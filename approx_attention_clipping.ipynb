{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random, math, time, sys, os, tqdm\n",
    "import numpy as np\n",
    "import numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120000/120000 [00:00<00:00, 216602.72it/s]\n",
      "100%|██████████| 7600/7600 [00:00<00:00, 211108.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Dataset Stat.: name:AG_NEWS, nclass:5, max_len:1012, avg_len:236.477525, count:120000\n",
      "Classification Dataset Stat.: name:AG_NEWS, nclass:5, max_len:892, avg_len:235.2992105263158, count:7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-4_H-256_A-4 were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer.__init__: Model initialized. model = bert-mini\n",
      "Trainer.load: Loading... saves/cls_bert-mini.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120000/120000 [00:00<00:00, 215437.58it/s]\n",
      "100%|██████████| 7600/7600 [00:00<00:00, 217143.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Dataset Stat.: name:AG_NEWS, nclass:5, max_len:1012, avg_len:236.477525, count:120000\n",
      "Classification Dataset Stat.: name:AG_NEWS, nclass:5, max_len:892, avg_len:235.2992105263158, count:7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-4_H-256_A-4 were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer.__init__: Model initialized. model = bert-mini\n",
      "Trainer.load: Loading... saves/cls_bert-mini.pth\n",
      "Trainer.load: saves/att_approx_16_bert-mini.pth\n",
      "approx trained 50000\n"
     ]
    }
   ],
   "source": [
    "from trainer.classification import Trainer\n",
    "from trainer.attention_approx import Trainer as ApproxTrainer\n",
    "batch_size = 8\n",
    "\n",
    "trainer = Trainer(batch_size=batch_size, model='bert-mini')\n",
    "trainer.load()\n",
    "trainer.model.eval()\n",
    "bert = trainer.model.bert\n",
    "fc = trainer.model.classifier\n",
    "batch = trainer.get_batch()\n",
    "test_batch = trainer.get_batch(test=False)\n",
    "\n",
    "approx_trainer = ApproxTrainer(batch_size=batch_size, factor=16, model=trainer.model_type)\n",
    "approx_trainer.load()\n",
    "approx_bert = approx_trainer.bert\n",
    "approx_bert = approx_bert.eval()\n",
    "print('approx trained', approx_trainer.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models.sparse_token' from 'f:\\\\Library\\\\discrete_edge_learning\\\\models\\\\sparse_token.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import models.sparse_token as sparse\n",
    "importlib.reload(sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_bert = sparse.SparseBertModel(bert.config)\n",
    "sparse_bert.to(trainer.device)\n",
    "sparse_bert.eval()\n",
    "sparse_bert.load_state_dict(bert.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([1, 2, 4, 1, 3, 4, 3, 3], device='cuda:0'),\n",
       "  tensor([1, 2, 4, 1, 3, 4, 3, 3], device='cuda:0')),\n",
       " (tensor([1, 2, 4, 1, 3, 4, 3, 3], device='cuda:0'),\n",
       "  tensor([1, 2, 4, 1, 3, 4, 3, 3], device='cuda:0')))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eval_fc(lm_output, fc=fc, batch=batch):\n",
    "    last_hidden = lm_output.last_hidden_state[:,0,:]\n",
    "    x = fc(last_hidden)\n",
    "    return torch.argmax(x, dim=-1), batch.labels, lm_output\n",
    "\n",
    "def eval(bert, fc=fc, batch=batch):\n",
    "    lm_output = bert(\n",
    "        input_ids = batch.input_ids, \n",
    "        attention_mask = batch.attention_masks,\n",
    "        output_hidden_states = True,\n",
    "        output_attentions = True,\n",
    "    )\n",
    "    return eval_fc(lm_output, fc=fc, batch=batch)\n",
    "\n",
    "def approx_eval(sparse_bert, approx_bert, fc=fc, batch=batch):\n",
    "    lm_output = sparse.run_bert_with_approx(\n",
    "        sparse_bert, \n",
    "        approx_bert, \n",
    "        {\n",
    "            'input_ids': batch.input_ids,\n",
    "            'attention_mask': batch.attention_masks,\n",
    "            'output_hidden_states': True,\n",
    "            'output_attentions': True,\n",
    "        },\n",
    "        ks = [0.25]*len(sparse_bert.encoder.layer),\n",
    "    )\n",
    "    return eval_fc(lm_output, fc=fc, batch=batch)\n",
    "    \n",
    "eval(bert)[:2], approx_eval(sparse_bert, approx_bert)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:01<00:00, 39.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approx_att                   : 0:00:00.309996 (16.79%)\n",
      "approx_mask_reset            : 0:00:00.013994 (0.76%)\n",
      "approx_mask_reset_inverse    : 0:00:00.006000 (0.32%)\n",
      "approx_mask_update           : 0:00:00.078007 (4.22%)\n",
      "approx_sparse                : 0:00:00.503007 (27.24%)\n",
      "bert.attention.output        : 0:00:00.069040 (3.74%)\n",
      "bert.attention.probs.dropout : 0:00:00.003007 (0.16%)\n",
      "bert.attention.qkv           : 0:00:00.154954 (8.39%)\n",
      "bert.attention.scores.matmul : 0:00:00.021003 (1.14%)\n",
      "bert.intermediate            : 0:00:00.069785 (3.78%)\n",
      "bert.output                  : 0:00:00.053211 (2.88%)\n",
      "sparselinear                 : 0:00:00.287205 (15.55%)\n",
      "sparselinear.gather          : 0:00:00.075998 (4.12%)\n",
      "sparselinear.linear          : 0:00:00.116000 (6.28%)\n",
      "sparselinear.scatter_        : 0:00:00.085203 (4.61%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9194444444444444, 0.9194444444444444, False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(sparse)\n",
    "def accuracy(batch_eval, N=720//16, return_lm=False):\n",
    "    trainer.seed()\n",
    "    trainer.dataset.batch_size = 16\n",
    "    acc_sum = 0\n",
    "    for i in tqdm.tqdm(range(N)):\n",
    "        batch = trainer.get_batch(test=True)\n",
    "        with torch.no_grad():\n",
    "            output, label, _ = batch_eval(batch)\n",
    "        acc_sum += torch.mean((output == label) * 1.0)\n",
    "    if return_lm: return acc_sum.item() / N, _\n",
    "    return acc_sum.item() / N\n",
    "\n",
    "# setup for evaluation\n",
    "sparse_bert = sparse.SparseBertModel(bert.config)\n",
    "sparse_bert.to(trainer.device)\n",
    "sparse_bert.eval()\n",
    "sparse_bert.load_state_dict(bert.state_dict())\n",
    "sparse.set_print(sparse_bert, False)\n",
    "sparse.set_backup_last_inputs(sparse_bert, False)\n",
    "sparse.set_output_masking(sparse_bert, False)\n",
    "\n",
    "sparse_bert = sparse_bert.to(trainer.device)\n",
    "approx_bert = approx_bert.to(trainer.device)\n",
    "bert = bert.to(trainer.device)\n",
    "sparse.set_print(sparse_bert, False)\n",
    "sparse.set_backup_last_inputs(sparse_bert, False)\n",
    "sparse.set_output_masking(sparse_bert, False)\n",
    "\n",
    "sparse.timer_reset()\n",
    "acc_approx, lm = accuracy(\n",
    "    lambda batch: approx_eval(sparse_bert, approx_bert, batch=batch),\n",
    "    return_lm = True,\n",
    ")\n",
    "acc_bert = acc_approx\n",
    "#acc_bert = accuracy(lambda batch: eval(bert, batch=batch))\n",
    "sparse.timer_report()\n",
    "acc_bert, acc_approx, abs(acc_approx - 0.9236111111111112) < 0.001\n",
    "#(0.934375, 0.92203125)\n",
    "#(0.934375, 0.93359375)\n",
    "# 0.8486328125\n",
    "\n",
    "#(0.9243055555555556, 0.9173611111111111, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD6CAYAAABnLjEDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAASp0lEQVR4nO3de4wd1X0H8O/33n15jR/YENd4DXbkVwwBu9mAqdOUmJI4YMWoQhEojYyE5KpNJGiQwGnatJHyh1NVIfkDFVkYxVURjxIkEEoUucYpTUpsbFgIxrG9Ni8TgwG/bbzrvffXP3a83nnszuy9M3Pn7vl+JMv3nDsz5zfX9+e5Zx7n0MwgIuNfqdEBiEg+lOwijlCyizhCyS7iCCW7iCOU7CKOqCvZSa4kuYdkL8l1aQUlIuljrdfZSZYB7AVwE4CDAF4CcIeZvTHSOm1stw5MrKm9RuibO8FXbn/zE1+5Mi28L+UjpzONSWQ0Z3Ea/dbHqPda6tjutQB6zewAAJB8HMBqACMmewcm4jreWEeTCTFiX+P+U4tYp/eHS3zleX/9iq98/JZloXWm/Ofv/BWlciCO6thjazbBz3K87V+tgt+FamXs24j5bm+zLSM3P/bWhswC8O6w8kGvTkQKqJ4jeyIk1wJYCwAd6My6OREZQT3J/h6A2cPKXV6dj5ltALABACZPmmW2ZMmFxj88GdpoZe/+OkIaajSVdeZ9s2fUVSa92xe/3Vp+qjU7/WyPlsZ3oY7Ptp6f8S8BmE9yLsk2ALcDeLaO7YlIhmo+spvZAMlvA/gVgDKAR8xsV2qRiUiq6uqzm9kvAPwipVhEJEO6g07EEZmfjQ+yYZcJ//C9KaH3F93tr6scO55Ow7Vc44w5GVJ6oSdc1+m/4mD9/f7ywEB8u0G13Dcg41J54bxQXWVPb6J1dWQXcYSSXcQRSnYRR+TaZ+fZc2jdc+G+m/l3fhRa5l8ObPeVv79gua9s5/x94MSyuLklot9cPXMml3bETUn751F0ZBdxhJJdxBFKdhFH5HudvVqFDevTBq9JA8A/X3WDr8w2//tRffbSxOYZECOJ6mn/ABhp7V9wu1kJxptXu0nU8lmmFX+StuPaitpG0vh0ZBdxhJJdxBFKdhFHKNlFHJHvCbr2NmD+FUPFas+IY1MOeXP99b7y3HUvhpapXO1/OIAvvlpjgH5s9Z8drPmGnjrVfIIo6gGaHBTphFxQZGyBz6k8aVJ+bee4DR3ZRRyhZBdxhJJdxBG59tk75pzFwo17h8q7Pxe/TlQfPaQU6JsGB6oAanoQplF99NToAZpkAp9T5WR41OPxQEd2EUco2UUcoWQXcYSSXcQR+Y5UQ6C9NGx01ZROpPG3Pb5y6ZrPhDf7+72BiuaZlokt4X8mqwTi18m49IzTz1JHdhFHKNlFHKFkF3FErn32oycn4qn/WTZUXtDWE1qGEwMzwnx8ZOwN7XkzVNV/01JfeULPO/52Pjgcu1m2t/srquG+nQ2cG30jNfQHE80io1ljJIaO7CKOULKLOELJLuKIXPvsLWeAaa9e6FtWz54NL9Tv7/OWp0/zlSP78IH+atR2J+z09+P337PAV57zTx+G1gm1/dHH4baLIqp/XsvMtTJu6cgu4gglu4gjlOwijohNdpKPkDxM8vVhddNIbia5z/v74mzDFJF60WJuvCD5RQCnAPyHmV3l1f0rgCNmtp7kOgAXm9n9cY1NXfQpu+Hh24bKp78YPikWktFJpvL8T/s3+9bB0DJsa/Uvk9WoqcEbYtK6GSar7Y5zwWnJ0pqGO9HJ5jptsy04YUcihxWOPbKb2QsAglGtBrDJe70JwK31BCgi2au1zz7DzA55r98HMGOkBUmuJbmD5I7+Y5/U2JyI1KvuE3Q22A8Y8fehmW0ws24z626bOqHe5kSkRrXeVPMByZlmdojkTADxT5EAsHda8Mm3pw+rSdBnT+tGkEDfv7L/bf/7Vg2t8qWXj/rKz3/WP11ucMYYoMYRabPqS6uPXpO0+uhBWfTRx6LWI/uzANZ4r9cAeCadcEQkK0kuvT0G4EUAC0keJHkXgPUAbiK5D8BfemURKbDYn/FmdscIb92YciwikqHY6+xpWnpNmz3/y08Nlb/edf0oS4vIWNV1nV1Exgclu4gjlOwijlCyizgi15Fqdp+6FJ//zd8Mledd2RdaprJrT54h1aV/5edDde1bX/OVrS+8jyKNoCO7iCOU7CKOULKLOCLXm2oml6bZspavDJUTzXRSZBGzsPzs7f/1le+8/Aux6+iBFUmLbqoRESW7iCuU7CKOyPU6OyZOQKX7s0PF0gs98etk1J8NDjyR1qATd17x5/5F/uxqX7ll/yEEJZlBVgQA9j14Xahu/re2JVpXR3YRRyjZRRyhZBdxhJJdxBG5nqCrtJVw6rL2ofLkBt5MkuiEXC0zqgSWaXk9MFX0fVeGVglNF62bbMallpl/4isPHHp/zNtY9ODRUF3S8Zd1ZBdxhJJdxBFKdhFH5PogzJTyJbasc1Vd24iaSbU0cWLEks0ruI9p7V9ms9AGBOPNq90kavks04o/SdtxbUVtY/g6ehBGRJTsIq5Qsos4Itfr7JMWncMNT1546CM4K2qtitQnrEX5yoX+igSDbhZ5nxVbdm3Xsw0d2UUcoWQXcYSSXcQRSnYRR+R6gu50pQ07j18+VGbryfBCJf/9AGnNqMIW/64WaWTbuFlweMWsUF3prYP+imo1tExwH4u0z1KjUjlcV032KIyO7CKOULKLOCI22UnOJrmV5Bskd5G826ufRnIzyX3e3xdnH66I1CpJn30AwL1m9jLJSQB2ktwM4E4AW8xsPcl1ANYBuH+0DVUOtuHYfV1DZVZeDy1TnjvHv87e/QlCjGeVpI/4F0/lovZQ3d5/u8pXXvB328MrRs0+I80tYf88SuyR3cwOmdnL3uuTAHYDmAVgNYBN3mKbANxacxQikrkx9dlJzgGwFMA2ADPM7Pwg6O8DmJFuaCKSpsTJTvIiAD8HcI+ZnRj+ng0+FB/5YDzJtSR3kNxxbqC490yLjHeJkp1kKwYT/VEze9qr/oDkTO/9mQAipzUxsw1m1m1m3a0t42uQCZFmEnuCjiQBbASw28x+POytZwGsAbDe+/uZ2NZOfwK++NqFcsQoOZV9B2I3UxMG/l+zJjph91L4ROZ1D/gvfnz8F0tDy5T/b5evXNMUV+NMcNovwJ3PJcnZ+OUAvgng9yR7vLp/wGCSP0nyLgBvA/h6JhGKSCpik93MfgNgpGs4N6YbjohkRXfQiTgi1wdhWCqh1NlZ1zacHF024jM7/mX/A0It2BveUFurr5hX37TIo8sy8JmMVDdcM40uO+q6iZYSkaanZBdxhJJdxBFKdhFHKNlFHKFkF3GEkl3EEUp2EUco2UUcoWQXcYSSXcQRSnYRRyjZRRyhZBdxhJJdxBFKdhFHKNlFHKFkF3GEkl3EEUp2EUco2UUcoWQXcYSSXcQRSnYRRyjZRRyR64ww1tmOypL5Q2X+tieV7WY24wgDU9xFzDqbhyLNqNLsknyWpY6OTNq+bIv/+3Rw2akxb6Oe74KO7CKOULKLOELJLuKIfGdxPdOHcs++CxU1zL7q5CyuKe2f+v6N/a788Ub/OZ9aZmTVLK4iEkvJLuIIJbuII2KTnWQHye0kXyW5i+QPvPq5JLeR7CX5BMm27MMVkVolObL3AVhhZtcAWAJgJcllAH4E4AEzmwfgKIC7MotSROoWm+w26PytPq3eHwOwAsBTXv0mALdmEaCIpCNRn51kmWQPgMMANgPYD+CYmQ14ixwEMGuEddeS3EFyR7+dTSFkEalFomQ3s4qZLQHQBeBaAIuSNmBmG8ys28y625jNPcciEm9MZ+PN7BiArQCuBzCV5PmbcroAvJduaCKSpiRn4y8lOdV7PQHATQB2YzDpb/MWWwPgmYxiFJEUJLlddiaATSTLGPzP4Ukze47kGwAeJ/lDAK8A2JhhnCJSp9hkN7PXACyNqD+Awf67iDQB3UEn4gglu4gjlOwijlCyizhCyS7iCCW7iCOU7CKOULKLOELJLuIIJbuII5TsIo5Qsos4Qsku4gglu4gjlOwijlCyizhCyS7iCCW7iCOU7CKOULKLOELJLuIIJbuII5KMG5+as7M78Id/XDxUXrD2pdAyZ/7qOl+58+ltsdutnj4du0ypwz/1VPVs88w7F7V/5Uum+ysq1dAyleMnsgppVEn+PRrFBgbCdX19ubSdxudSzzZ0ZBdxhJJdxBFKdhFHKNlFHEEzy62xKaXptqzj5qFyM50kK7rSxIkRlf7/y6snT+YUTXH97b7eUN1Dn1nkK1ul4l8gxxyp1zbbghN2hFHv6cgu4gglu4gjlOwijsj3ppquTuy9d8lQed7f/y5+JQa6H03Uf8pT1M0WbG9vQCTF9tDixaG6Y7f/qa88bfuHvnJl7/5U2i51dvrK1TNnUtlu4vZzbU1EGkbJLuKIxMlOskzyFZLPeeW5JLeR7CX5BMm27MIUkXolvs5O8jsAugFMNrNVJJ8E8LSZPU7yIQCvmtm/j7aNKeVLbFnnqroCjuqbRl5jbmLBfUxr//J6QCUYb5EejKnls0wr/iRtx7UVtY3h69R9nZ1kF4BbADzslQlgBYCnvEU2Abg1ybZEpDGS/oz/CYD7AJx/jnI6gGNmdv55wYMAZkWtSHItyR0kd/Sb7pgTaZTYZCe5CsBhM9tZSwNmtsHMus2su40d8SuISCaSXGdfDuBrJG8G0AFgMoCfAphKssU7uncBeC+7MEWkXrFHdjP7rpl1mdkcALcDeN7MvgFgK4DbvMXWAHgmsyhFpG71XGe/H8B3SPZisA+/MZ2QRCQLY7pd1sx+DeDX3usDAK5NPyQRyYLuoBNxhJJdxBFKdhFHKNlFHKFkF3GEkl3EEUp2EUco2UUcoWQXcYSSXcQRuY4uCxJsuzB6VeXo0VQ2m8ZIImwNj6pl5/rr3m64oYhBRGJGCyrSSC9JFDneyOmvp0/zlStH0vleBt2xc4+v/OiirjFvQ1M2i0gsJbuII5TsIo7It89uBuu/0A9Oa6TPrEaXZVtrJtuNo9FlsxP1WdrZPv8ywZlbUor/sc8tDMQSXqbe0WVHXTfRUiLS9JTsIo5Qsos4Qsku4ohcT9BZtZrJyZoinQDKQrPtX5HjbWRswbZ/9cee0DJfuWzJmLYxFjqyizhCyS7iCCW7iCNy7bOzVEKps74bRDRlc3rbzUqz3VQTJ6spm786f3nEUrqpRkTqpGQXcYSSXcQR+T4I01JG6ZILAwVU3jsUWqQ0ZbKvXPn4SOxmi9QnjBoEYzirVMKV1Yi64W8XaP+SKHK8RbrOzvb2MW/j8q3VUN1bCWdc1JFdxBFKdhFHKNlFHKFkF3FEvifoBiqofnThhFvUCYrQqCEJbtBopptqiPjRb3RTTXaKdFNNFOvrG/X9d75U+/FZR3YRRyjZRRyhZBdxBC1mNpJUGyM/BPA2gEsAfJRbw/VppliB5oq3mWIFmiPeK8zs0qg3ck32oUbJHWbWnXvDNWimWIHmireZYgWaL94g/YwXcYSSXcQRjUr2DQ1qtxbNFCvQXPE2U6xA88Xr05A+u4jkTz/jRRyRa7KTXElyD8lekuvybDsJko+QPEzy9WF100huJrnP+/viRsZ4HsnZJLeSfIPkLpJ3e/VFjbeD5HaSr3rx/sCrn0tym/edeILk6AMC5IhkmeQrJJ/zyoWNNYnckp1kGcCDAL4KYDGAO0guzqv9hH4GYGWgbh2ALWY2H8AWr1wEAwDuNbPFAJYB+Jb3eRY13j4AK8zsGgBLAKwkuQzAjwA8YGbzABwFcFfjQgy5G8DuYeUixxorzyP7tQB6zeyAmfUDeBzA6hzbj2VmLwAIDo2zGsAm7/UmALfmGdNIzOyQmb3svT6JwS/lLBQ3XjOzU16x1ftjAFYAeMqrL0y8JLsA3ALgYa9MFDTWpPJM9lkA3h1WPujVFd0MMzs/ftb7AGY0MpgoJOcAWApgGwocr/ezuAfAYQCbAewHcMzMBrxFivSd+AmA+wCcHwdqOoobayI6QTcGNnjpolCXL0heBODnAO4xsxPD3ytavGZWMbMlALow+EtvUWMjikZyFYDDZraz0bGkKc/n2d8DMHtYucurK7oPSM40s0MkZ2LwqFQIJFsxmOiPmtnTXnVh4z3PzI6R3ArgegBTSbZ4R8yifCeWA/gayZsBdACYDOCnKGasieV5ZH8JwHzvjGYbgNsBPJtj+7V6FsAa7/UaAM80MJYhXh9yI4DdZvbjYW8VNd5LSU71Xk8AcBMGzzNsBXCbt1gh4jWz75pZl5nNweD39Hkz+wYKGOuYmFlufwDcDGAvBvtq38uz7YTxPQbgEIBzGOyT3YXBvtoWAPsA/DeAaY2O04v1Cxj8if4agB7vz80FjvdqAK948b4O4Pte/acBbAfQC+C/ALQ3OtZA3DcAeK4ZYo37ozvoRByhE3QijlCyizhCyS7iCCW7iCOU7CKOULKLOELJLuIIJbuII/4fjJpTvPpN6V0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.4880e-03, 4.4412e-04, 3.4626e-04, 3.1635e-03, 0.0000e+00, 3.1504e-03,\n",
      "         6.0098e-03],\n",
      "        [1.7687e-01, 1.1223e-01, 1.0283e-03, 4.4937e-06, 0.0000e+00, 7.9757e-04,\n",
      "         1.9282e-03]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_grid(grid):\n",
    "    plt.imshow(grid.cpu().detach().numpy())\n",
    "    #plt.colorbar()\n",
    "    plt.show()\n",
    "plot_grid(lm.attentions[-2][0][0][:50, :50])\n",
    "print(lm.attentions[-2][0][0][:2, :7])\n",
    "#tensor([[0.0246, 0.0012, 0.0206, 0.0003, 0.0000, 0.0002, 0.0049]],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(eval, batch_size=8, N=100, WARM=20, amp=False, device=trainer.device, end_warm=None):\n",
    "    assert WARM < (N * 0.33)\n",
    "    trainer.dataset.batch_size = batch_size\n",
    "    batch = trainer.get_batch(test=True)\n",
    "    batch = batch.to(device)\n",
    "    assert batch.input_ids.shape[0] == batch_size\n",
    "    for i in tqdm.tqdm(range(N)):\n",
    "        if i == WARM: \n",
    "            t = time.time()\n",
    "            if not end_warm is None: end_warm()\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(enabled=amp):\n",
    "            eval(batch)\n",
    "    t = time.time() - t\n",
    "    t_item = t / (batch_size * (N-WARM))\n",
    "    return t, t_item * 1000, 1.0/t_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:01<00:00, 14.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approx_att                   : 0:00:00.530008 (16.51%)\n",
      "approx_mask_reset            : 0:00:00.009992 (0.31%)\n",
      "approx_mask_reset_inverse    : 0:00:00.002000 (0.06%)\n",
      "approx_mask_update           : 0:00:00.082006 (2.55%)\n",
      "approx_sparse                : 0:00:00.917022 (28.56%)\n",
      "bert.attention.output        : 0:00:00.162011 (5.05%)\n",
      "bert.attention.probs.dropout : 0:00:00.001000 (0.03%)\n",
      "bert.attention.qkv           : 0:00:00.240004 (7.47%)\n",
      "bert.attention.scores.matmul : 0:00:00.048999 (1.53%)\n",
      "bert.intermediate            : 0:00:00.111025 (3.46%)\n",
      "bert.output                  : 0:00:00.100998 (3.15%)\n",
      "sparselinear                 : 0:00:00.507026 (15.79%)\n",
      "sparselinear.gather          : 0:00:00.104996 (3.27%)\n",
      "sparselinear.linear          : 0:00:00.285007 (8.88%)\n",
      "sparselinear.scatter_        : 0:00:00.109022 (3.40%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.556016445159912, 2.2102506323294207, 452.43737763173175)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(sparse)\n",
    "sparse.timer_reset()\n",
    "benchmark_device = 'cuda'\n",
    "benchmark_batch_size = 32\n",
    "\n",
    "sparse_bert = sparse.SparseBertModel(bert.config)\n",
    "sparse_bert.to(benchmark_device)\n",
    "sparse_bert.eval()\n",
    "sparse_bert.load_state_dict(bert.state_dict())\n",
    "sparse.set_print(sparse_bert, False)\n",
    "sparse.set_backup_last_inputs(sparse_bert, False)\n",
    "sparse.set_output_masking(sparse_bert, False)\n",
    "\n",
    "sparse_bert=sparse_bert.to(benchmark_device)\n",
    "approx_bert=approx_bert.to(benchmark_device)\n",
    "time_approx = benchmark(\n",
    "    eval = lambda batch: approx_eval(sparse_bert, approx_bert, batch=batch, fc=lambda x: x),\n",
    "    batch_size = benchmark_batch_size,\n",
    "    WARM = 3,\n",
    "    N = 25,\n",
    "    device = benchmark_device,\n",
    "    end_warm = lambda: sparse.timer_reset()\n",
    ")\n",
    "sparse.timer_report()\n",
    "time_approx\n",
    "\n",
    "# bert.attention.output        : 0:00:00.545988 (3.04%)\n",
    "# bert.attention.probs.dropout : 0:00:00.006996 (0.04%)\n",
    "# bert.attention.qkv           : 0:00:01.473990 (8.20%)\n",
    "# bert.attention.scores.matmul : 0:00:00.113033 (0.63%)\n",
    "# bert.intermediate            : 0:00:02.113082 (11.75%)\n",
    "# bert.output                  : 0:00:01.848977 (10.28%)\n",
    "# sparselinear.linear          : 0:00:03.801073 (21.14%)\n",
    "\n",
    "# bert.attention.output        : 0:00:01.103962 (2.75%)\n",
    "# bert.attention.probs.dropout : 0:00:00.010004 (0.02%)\n",
    "# bert.attention.qkv           : 0:00:02.440944 (6.07%)\n",
    "# bert.attention.scores.matmul : 0:00:00.534112 (1.33%)\n",
    "# bert.intermediate            : 0:00:02.811061 (6.99%)\n",
    "# bert.output                  : 0:00:02.548062 (6.34%)\n",
    "# sparselinear                 : 0:00:07.770034 (19.33%)\n",
    "# sparselinear.gather          : 0:00:01.343004 (3.34%)\n",
    "# sparselinear.linear          : 0:00:04.463333 (11.11%)\n",
    "# sparselinear.scatter_        : 0:00:01.947706 (4.85%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:01<00:00, 12.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.742016315460205, 2.474454993551428, 404.12939520260323)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert = bert.to(benchmark_device)\n",
    "time_bert = benchmark(\n",
    "    lambda batch: eval(bert, batch=batch, fc=lambda x: x), \n",
    "    batch_size = benchmark_batch_size,\n",
    "    WARM = 3,\n",
    "    N=25,\n",
    "    device = benchmark_device\n",
    ")\n",
    "time_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2,2).unsqueeze(-1).repeat(1, 1, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7b3ac0126d0d6fea024471ce24e510948bf6332f7ae1a66cdcb4ee9887514e9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('tensorflow': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
