{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAELklEQVR4nO3YPY5bVQCG4XOvPZNMAlIoaCIhgURLAS0sISWiZ1fZBqIi7IASKjaBEkVOxvNnNjAZGtv3xXme8p7mk+z3XMvTbrcbQM+89ADgfuKEKHFClDghSpwQtX7o8PnL70/yr9yfvnsxfvz2xdIz9m63242bu5ulZxzEPM1jNa+WnnEQP5x9M9333JsTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghav3Q4deff3mkGcd1cXYx/tm8XnrG3k3TNB6vz8c0TUtP2bvL2+24vr1ZesZhPLv/8X/E+dUBlizv4uzxeP3+zdIz9m6e5rF++mys5tXSU/bu3dX7sbl6t/SMw3h2/2M/ayFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0StHzp8cn5xrB1H9Wh9PlbT6d1L8zyPaZrHNKalp+zd1c3VePP+7dIzjmra7XYfPHy1+ePDh/9j0xhjmk7vCzzGNM7XZycZ5y9//jZ+/ev3pWccxN8/v7r3A/so35y3d7fjbne39IyDmMZ0khfP9uZ6vN1ulp5xVKf32w5OhDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IWj90uL2+OtaOo7q82Y7L68ulZ+zdPM3j08dPx3SCd+4nj56MLz57vvSMo3owzuvb62PtOKrNdjPebjdLz9i7eZrH+epsrObV0lP27mOM8/SuWDgR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBA17Xa7pTcA9/DmhChxQpQ4IUqcECVOiBInRP0LvTpV4VCKmrwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABiCAYAAADz0wB7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAABv0lEQVR4nO3aIU5EMQBF0X5mPgI1QZGwDLaGZgNYVoZD4CE4DCSIsgJQ5P4wc46teaK5qegy5xwANM62HgBwSkQXICS6ACHRBQiJLkBIdAFC+98O7x4f/Cf7I5cXh3F9uBrnu3XrKUfhfL+OdbeOZVm2nvLvfXx9jqfX5/Hy/rb1lKNxf3P748X00gUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQsucc+sNACfDSxcgJLoAIdEFCIkuQEh0AUKiCxD6Bv2sE78zwaW6AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABiCAYAAADz0wB7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAABs0lEQVR4nO3aIU5DQRhG0XmkJCCxTdAoNMtCdwVoloWuwhOwKEhqBo0oqrkvoefYMZ+6+cUsc84BQONi7QEA50R0AUKiCxASXYCQ6AKERBcgtPnr8fHlyX+yE7m92Y777d24vrxaewr88nX4Hvv31/H2+bH2lH/j+WG3HHtz6QKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAIdEFCIkuQEh0AUKiCxASXYCQ6AKERBcgJLoAoWXOufYGgLPh0gUIiS5ASHQBQqILEBJdgJDoAoR+AP8ME78QfdZyAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for dummy attention\n",
    "\n",
    "def imshow_attention(mat):\n",
    "    if isinstance(mat, torch.Tensor):\n",
    "        mat = mat.detach().cpu().numpy()\n",
    "    mat = cv2.resize(mat, None, fx=32, fy=32, interpolation=cv2.INTER_NEAREST)\n",
    "    H, W = mat.shape\n",
    "    mat = mat.reshape(H, W, 1)\n",
    "    high_color = np.array([10, 143, 45]).reshape(1,1,3)\n",
    "    low_color = np.array([199, 252, 213]).reshape(1,1,3)\n",
    "    mat = mat*high_color + (1-mat)*low_color\n",
    "    mat = mat.astype(np.uint8)\n",
    "    plt.imshow(mat)\n",
    "    # plt.yticks([])\n",
    "    # plt.xticks([])\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "score = torch.tensor([\n",
    "    [5, 1, 2, 1],\n",
    "    [3, 2, 1, 2],\n",
    "    [1, 2, 1, 5],\n",
    "    [1, 2, 1, 3],\n",
    "], dtype=torch.float32)\n",
    "prob = torch.softmax(score, dim=-1)\n",
    "reduced_prob = torch.mean(prob, dim=0, keepdim=True)\n",
    "\n",
    "imshow_attention(prob)\n",
    "imshow_attention(reduced_prob)\n",
    "imshow_attention(torch.mean(prob[[0,2],:], dim=0, keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base\t22.4 GFLOPs\t100%\n",
      "approx-bert@f4\t1.5 GFLOPs\t6.86%\n",
      "approx-bert@f8\t436.9 MFLOPs\t1.95%\n",
      "approx-bert@f16\t140.6 MFLOPs\t0.63%\n",
      "fwd-overhead\t2.8 MFLOPs\t0.01%\n",
      "abt-oh@0.2\t665.5 KFLOPs\t0.00%\n",
      "abt-oh@0.4\t1.2 MFLOPs\t0.01%\n",
      "abt-oh@1.0\t2.9 MFLOPs\t0.01%\n",
      "conc-oh@0.2\t10.4 MFLOPs\t0.05%\n",
      "conc-oh@0.4\t19.6 MFLOPs\t0.09%\n",
      "conc-oh@1.0\t47.1 MFLOPs\t0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_615774/3293657249.py:115: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  df.to_latex('./saves_plot/table_overhead_flops.tex')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>FLOPs</th>\n",
       "      <th>Relative %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BERT_{base}</th>\n",
       "      <th></th>\n",
       "      <td>22.4 GFLOPs</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Attention Approx. Net.</th>\n",
       "      <th>factor=4</th>\n",
       "      <td>1.5 GFLOPs</td>\n",
       "      <td>6.86%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>factor=8</th>\n",
       "      <td>436.9 MFLOPs</td>\n",
       "      <td>1.95%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>factor=16</th>\n",
       "      <td>140.6 MFLOPs</td>\n",
       "      <td>0.63%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overhead of Manual Top-k</th>\n",
       "      <th></th>\n",
       "      <td>2.8 MFLOPs</td>\n",
       "      <td>0.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Overhead of Attention Back-tracking</th>\n",
       "      <th>0.2</th>\n",
       "      <td>665.5 KFLOPs</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.4</th>\n",
       "      <td>1.2 MFLOPs</td>\n",
       "      <td>0.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>2.9 MFLOPs</td>\n",
       "      <td>0.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Overhead of Concrete Masking</th>\n",
       "      <th>0.2</th>\n",
       "      <td>10.4 MFLOPs</td>\n",
       "      <td>0.05%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.4</th>\n",
       "      <td>19.6 MFLOPs</td>\n",
       "      <td>0.09%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>47.1 MFLOPs</td>\n",
       "      <td>0.21%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      FLOPs Relative %\n",
       "BERT_{base}                                     22.4 GFLOPs       100%\n",
       "Attention Approx. Net.              factor=4     1.5 GFLOPs      6.86%\n",
       "                                    factor=8   436.9 MFLOPs      1.95%\n",
       "                                    factor=16  140.6 MFLOPs      0.63%\n",
       "Overhead of Manual Top-k                         2.8 MFLOPs      0.01%\n",
       "Overhead of Attention Back-tracking 0.2        665.5 KFLOPs      0.00%\n",
       "                                    0.4          1.2 MFLOPs      0.01%\n",
       "                                    1.0          2.9 MFLOPs      0.01%\n",
       "Overhead of Concrete Masking        0.2         10.4 MFLOPs      0.05%\n",
       "                                    0.4         19.6 MFLOPs      0.09%\n",
       "                                    1.0         47.1 MFLOPs      0.21%"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flops table\n",
    "import copy, random\n",
    "import pandas as pd\n",
    "from utils import sparse_flops_calculation as calc\n",
    "\n",
    "SEQ = 128\n",
    "FACTORS = [4,8,16]\n",
    "OCCUPIES = [0.2, 0.4, 1.0]\n",
    "SAMPLES = 1000\n",
    "\n",
    "cols = []\n",
    "\n",
    "base_config = calc.ModelConfig(\n",
    "    num_layer=12,\n",
    "    num_heads=12,\n",
    "    hidden_size=768,\n",
    "    intermediate_size=768*4,\n",
    "    seq_len=SEQ,\n",
    "    arch='bert',\n",
    "    token_occupies=None\n",
    ")\n",
    "\n",
    "flops_bert_base = calc.flops_sparse_bert_model(base_config)\n",
    "print('bert-base', calc.human_readable(flops_bert_base), '100%', sep='\\t')\n",
    "cols.append((('BERT_{base}', ''), [calc.human_readable(flops_bert_base), '100%']))\n",
    "\n",
    "for factor in FACTORS:\n",
    "    config = copy.deepcopy(base_config)\n",
    "    config.hidden_size /= factor\n",
    "    config.intermediate_size /= factor\n",
    "    flops_approx_factor = calc.flops_sparse_bert_model(config)\n",
    "    print(\n",
    "        f'approx-bert@f{factor}', \n",
    "        calc.human_readable(flops_approx_factor), \n",
    "        f'{flops_approx_factor/flops_bert_base*100:.2f}%', \n",
    "        sep='\\t'\n",
    "    )\n",
    "    cols.append((\n",
    "        ('Attention Approx. Net.', f'factor={factor}'), \n",
    "        [\n",
    "            calc.human_readable(flops_approx_factor), \n",
    "            f'{flops_approx_factor/flops_bert_base*100:.2f}%',\n",
    "        ]\n",
    "    ))\n",
    "\n",
    "config = copy.deepcopy(base_config)\n",
    "config.sparse_mode = 'forward'\n",
    "config.token_occupies = [1.0 for _ in range(config.num_layer+1)]\n",
    "flops_forward_factor_overhead = calc.flops_sparse_update(config)\n",
    "print('fwd-overhead', \n",
    "    calc.human_readable(flops_forward_factor_overhead),\n",
    "    f'{flops_forward_factor_overhead/flops_bert_base*100:.2f}%', \n",
    "    sep='\\t'\n",
    ")\n",
    "cols.append((\n",
    "    ('Overhead of Manual Top-k', f''), \n",
    "    [\n",
    "        calc.human_readable(flops_forward_factor_overhead),\n",
    "        f'{flops_forward_factor_overhead/flops_bert_base*100:.2f}%', \n",
    "    ]\n",
    "))\n",
    "\n",
    "for occupy in OCCUPIES:\n",
    "    flops = 0\n",
    "    for _ in range(5000):\n",
    "        config = copy.deepcopy(base_config)\n",
    "        config.approx_hidden_size = config.hidden_size / 4\n",
    "        config.approx_intermediate_size = config.intermediate_size / 4\n",
    "        config.sparse_mode = 'approx'\n",
    "        config.token_occupies = [random.random() * (0.05+2*(occupy-0.05)) + 0.05 for _ in range(config.num_layer+1)]\n",
    "        flops += calc.flops_sparse_update(config)\n",
    "    flops /= 5000\n",
    "    print(f'abt-oh@{occupy}', \n",
    "        calc.human_readable(flops),\n",
    "        f'{flops/flops_bert_base*100:.2f}%', \n",
    "        sep='\\t'\n",
    "    )\n",
    "    cols.append((\n",
    "        ('Overhead of Attention Back-tracking', f'{occupy}'), \n",
    "        [\n",
    "            calc.human_readable(flops),\n",
    "            f'{flops/flops_bert_base*100:.2f}%', \n",
    "        ]\n",
    "    ))\n",
    "\n",
    "for occupy in OCCUPIES:\n",
    "    flops = 0\n",
    "    for _ in range(5000):\n",
    "        config = copy.deepcopy(base_config)\n",
    "        config.approx_hidden_size = config.hidden_size / 4\n",
    "        config.approx_intermediate_size = config.intermediate_size / 4\n",
    "        config.sparse_mode = 'concrete'\n",
    "        config.token_occupies = [random.random() * (0.05+2*(occupy-0.05)) + 0.05 for _ in range(config.num_layer+1)]\n",
    "        flops += calc.flops_sparse_update(config)\n",
    "    flops /= 5000\n",
    "    print(f'conc-oh@{occupy}', \n",
    "        calc.human_readable(flops),\n",
    "        f'{flops/flops_bert_base*100:.2f}%', \n",
    "        sep='\\t'\n",
    "    )\n",
    "    cols.append((\n",
    "        ('Overhead of Concrete Masking', f'{occupy}'), \n",
    "        [\n",
    "            calc.human_readable(flops),\n",
    "            f'{flops/flops_bert_base*100:.2f}%', \n",
    "        ]\n",
    "    ))\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for header, content in cols:\n",
    "    df[header] = content\n",
    "df.columns = pd.MultiIndex.from_tuples([(c[0], c[1]) for c in df.columns])\n",
    "df.index = ['FLOPs', 'Relative %']\n",
    "df = df.transpose()\n",
    "df.to_latex('./saves_plot/table_overhead_flops.tex')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58c896f8fe28377dc6f47dbc9814b9367447c8ff4b1090ace6962dd6db7d2533"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
