{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trainer.glue_base as glue_base\n",
    "import models.sparse_token as sparse\n",
    "import pickle, importlib\n",
    "importlib.reload(glue_base)\n",
    "importlib.reload(sparse)\n",
    "Glue = glue_base.GlueAttentionApproxTrainer\n",
    "PICKLE_PATH = \"glue_benchmark_wiki.pkl\"\n",
    "TEX_PATH = \"saves_plot/glue_benchmark_wiki.tex\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer: cola\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ainl/library/discrete_edge_learning/plot_benchmark_glue_wiki.ipynb Cell 2'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ainl/library/discrete_edge_learning/plot_benchmark_glue_wiki.ipynb#ch0000001?line=18'>19</a>\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ainl/library/discrete_edge_learning/plot_benchmark_glue_wiki.ipynb#ch0000001?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m subset \u001b[39min\u001b[39;00m subsets:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ainl/library/discrete_edge_learning/plot_benchmark_glue_wiki.ipynb#ch0000001?line=20'>21</a>\u001b[0m     trainer \u001b[39m=\u001b[39m Glue(dataset\u001b[39m=\u001b[39;49msubset, factor\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, device\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, wiki_train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ainl/library/discrete_edge_learning/plot_benchmark_glue_wiki.ipynb#ch0000001?line=21'>22</a>\u001b[0m     trainer\u001b[39m.\u001b[39mload()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ainl/library/discrete_edge_learning/plot_benchmark_glue_wiki.ipynb#ch0000001?line=22'>23</a>\u001b[0m     scores \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/library/discrete_edge_learning/trainer/glue_base.py:117\u001b[0m, in \u001b[0;36mGlueAttentionApproxTrainer.__init__\u001b[0;34m(self, dataset, factor, batch_size, device, wiki_train, wiki_epochs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ainl/library/discrete_edge_learning/trainer/glue_base.py?line=113'>114</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size \u001b[39m=\u001b[39m batch_size\n\u001b[1;32m    <a href='file:///home/ainl/library/discrete_edge_learning/trainer/glue_base.py?line=114'>115</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m device\n\u001b[0;32m--> <a href='file:///home/ainl/library/discrete_edge_learning/trainer/glue_base.py?line=116'>117</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m get_base_model(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset)\n\u001b[1;32m    <a href='file:///home/ainl/library/discrete_edge_learning/trainer/glue_base.py?line=117'>118</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n\u001b[1;32m    <a href='file:///home/ainl/library/discrete_edge_learning/trainer/glue_base.py?line=118'>119</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/library/discrete_edge_learning/trainer/glue_base.py:97\u001b[0m, in \u001b[0;36mget_base_model\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     <a href='file:///home/ainl/library/discrete_edge_learning/trainer/glue_base.py?line=72'>73</a>\u001b[0m checkpoint \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='file:///home/ainl/library/discrete_edge_learning/trainer/glue_base.py?line=73'>74</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcola\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mtextattack/bert-base-uncased-CoLA\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///home/ainl/library/discrete_edge_learning/trainer/glue_base.py?line=74'>75</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmnli\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39myoshitomo-matsubara/bert-base-uncased-mnli\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/ainl/library/discrete_edge_learning/trainer/glue_base.py?line=81'>82</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mwnli\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mtextattack/bert-base-uncased-WNLI\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='file:///home/ainl/library/discrete_edge_learning/trainer/glue_base.py?line=82'>83</a>\u001b[0m }[dataset]\n\u001b[1;32m     <a href='file:///home/ainl/library/discrete_edge_learning/trainer/glue_base.py?line=84'>85</a>\u001b[0m model \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='file:///home/ainl/library/discrete_edge_learning/trainer/glue_base.py?line=85'>86</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcola\u001b[39m\u001b[39m\"\u001b[39m: berts\u001b[39m.\u001b[39mBertForSequenceClassification,\n\u001b[1;32m     <a href='file:///home/ainl/library/discrete_edge_learning/trainer/glue_base.py?line=86'>87</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmnli\u001b[39m\u001b[39m\"\u001b[39m: berts\u001b[39m.\u001b[39mBertForSequenceClassification,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/ainl/library/discrete_edge_learning/trainer/glue_base.py?line=93'>94</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mwnli\u001b[39m\u001b[39m\"\u001b[39m: berts\u001b[39m.\u001b[39mBertForSequenceClassification,\n\u001b[1;32m     <a href='file:///home/ainl/library/discrete_edge_learning/trainer/glue_base.py?line=94'>95</a>\u001b[0m }[dataset]\n\u001b[0;32m---> <a href='file:///home/ainl/library/discrete_edge_learning/trainer/glue_base.py?line=96'>97</a>\u001b[0m bert \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfrom_pretrained(checkpoint)\n\u001b[1;32m     <a href='file:///home/ainl/library/discrete_edge_learning/trainer/glue_base.py?line=97'>98</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39mBertTokenizerFast\u001b[39m.\u001b[39mfrom_pretrained(checkpoint)\n\u001b[1;32m    <a href='file:///home/ainl/library/discrete_edge_learning/trainer/glue_base.py?line=99'>100</a>\u001b[0m \u001b[39mreturn\u001b[39;00m bert, tokenizer\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/transformers/modeling_utils.py:1435\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/transformers/modeling_utils.py?line=1432'>1433</a>\u001b[0m \u001b[39mif\u001b[39;00m state_dict \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/transformers/modeling_utils.py?line=1433'>1434</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/transformers/modeling_utils.py?line=1434'>1435</a>\u001b[0m         state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(resolved_archive_file, map_location\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/transformers/modeling_utils.py?line=1435'>1436</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/transformers/modeling_utils.py?line=1436'>1437</a>\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py:608\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py?line=605'>606</a>\u001b[0m             \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mload(opened_file)\n\u001b[1;32m    <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py?line=606'>607</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m--> <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py?line=607'>608</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py:787\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py?line=784'>785</a>\u001b[0m unpickler \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mUnpickler(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m    <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py?line=785'>786</a>\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m--> <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py?line=786'>787</a>\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m    <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py?line=788'>789</a>\u001b[0m deserialized_storage_keys \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m    <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py?line=790'>791</a>\u001b[0m offset \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mtell() \u001b[39mif\u001b[39;00m f_should_read_directly \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py:729\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py?line=726'>727</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpersistent_load\u001b[39m(saved_id):\n\u001b[1;32m    <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py?line=727'>728</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(saved_id, \u001b[39mtuple\u001b[39m)\n\u001b[0;32m--> <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py?line=728'>729</a>\u001b[0m     typename \u001b[39m=\u001b[39m _maybe_decode_ascii(saved_id[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m    <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py?line=729'>730</a>\u001b[0m     data \u001b[39m=\u001b[39m saved_id[\u001b[39m1\u001b[39m:]\n\u001b[1;32m    <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py?line=731'>732</a>\u001b[0m     \u001b[39mif\u001b[39;00m typename \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmodule\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py?line=732'>733</a>\u001b[0m         \u001b[39m# Ignore containers that don't have any sources saved\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py:803\u001b[0m, in \u001b[0;36m_maybe_decode_ascii\u001b[0;34m(bytes_str)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py?line=797'>798</a>\u001b[0m     torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m    <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py?line=799'>800</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m--> <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py?line=802'>803</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_maybe_decode_ascii\u001b[39m(bytes_str: Union[\u001b[39mbytes\u001b[39m, \u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py?line=803'>804</a>\u001b[0m     \u001b[39m# When using encoding='bytes' in Py3, some **internal** keys stored as\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py?line=804'>805</a>\u001b[0m     \u001b[39m# strings in Py2 are loaded as bytes. This function decodes them with\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py?line=805'>806</a>\u001b[0m     \u001b[39m# ascii encoding, one that Py3 uses by default.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py?line=806'>807</a>\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py?line=807'>808</a>\u001b[0m     \u001b[39m# NOTE: This should only be used on internal keys (e.g., `typename` and\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py?line=808'>809</a>\u001b[0m     \u001b[39m#       `location` in `persistent_load` below!\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py?line=809'>810</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(bytes_str, \u001b[39mbytes\u001b[39m):\n\u001b[1;32m    <a href='file:///home/ainl/anaconda3/envs/torch/lib/python3.8/site-packages/torch/serialization.py?line=810'>811</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m bytes_str\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "subsets = [\"cola\",\"mnli\",\"mrpc\",\"qnli\",\"qqp\",\"rte\",\"sst2\",\"stsb\",\"wnli\",]\n",
    "kss = [\n",
    "    0.1, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 0.999, 'dynamic', \n",
    "    'dynamic:avg:avg:true', 'dynamic:avg:avg:false', 'dynamic:avg:max:true', 'dynamic:avg:max:false',\n",
    "    'dynamic:max:avg:true', 'dynamic:max:avg:false', 'dynamic:max:max:true', 'dynamic:max:max:false',\n",
    "]\n",
    "kss = ['dynamic']\n",
    "sparse.benchmark_reset()\n",
    "# subsets = [\"mrpc\"]\n",
    "# kss = ['dynamic:avg:avg:f',0.1]\n",
    "\n",
    "def get_score(score):\n",
    "    if 'accuracy' in score:\n",
    "        return score['accuracy'], \"acc\"\n",
    "    first_metric = list(score.keys())[0]\n",
    "    return score[first_metric], first_metric\n",
    "\n",
    "results = {}\n",
    "i = 0\n",
    "for subset in subsets:\n",
    "    trainer = Glue(dataset=subset, factor=16, batch_size=-1, device=0, wiki_train=True)\n",
    "    trainer.load()\n",
    "    scores = {}\n",
    "    metric_name = \"\"\n",
    "    bert_score, metric_name = get_score(trainer.eval_base_model())\n",
    "    scores['bert'] = f'{bert_score:.5f}'\n",
    "    for ks in kss:\n",
    "        sparse.benchmark_reset()\n",
    "        sparse_score, _ = get_score(trainer.eval_sparse_model(ks=ks))\n",
    "        if isinstance(ks, str) and ks.startswith('dynamic'):\n",
    "            est_k = sparse.benchmark_get_average('est_k')\n",
    "            scores[str(ks)] = f'{sparse_score:.5f} (k:{est_k:.2f})'\n",
    "        else:\n",
    "            scores[str(ks)] = f'{sparse_score:.5f}'\n",
    "        i += 1\n",
    "        count = len(subsets) * len(kss)\n",
    "        print(f'{i}/{count}')\n",
    "    results[f\"{subset} ({metric_name})\"] = scores\n",
    "\n",
    "with open(PICKLE_PATH, 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "sparse.benchmark_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_159507/610143257.py:54: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  tex = df.to_latex()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cola</th>\n",
       "      <th>mnli</th>\n",
       "      <th>mrpc</th>\n",
       "      <th>qnli</th>\n",
       "      <th>qqp</th>\n",
       "      <th>rte</th>\n",
       "      <th>sst2</th>\n",
       "      <th>stsb</th>\n",
       "      <th>wnli</th>\n",
       "      <th>reproduce</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bert</th>\n",
       "      <td>0.53388</td>\n",
       "      <td>0.84198</td>\n",
       "      <td>0.84406</td>\n",
       "      <td>0.91543</td>\n",
       "      <td>0.90908</td>\n",
       "      <td>0.72563</td>\n",
       "      <td>0.92431</td>\n",
       "      <td>0.88047</td>\n",
       "      <td>0.56338</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dynamic:original</th>\n",
       "      <td>0.53388 (k:0.48)</td>\n",
       "      <td>0.84004 (k:0.33)</td>\n",
       "      <td>0.83942 (k:0.71)</td>\n",
       "      <td>0.91287 (k:0.44)</td>\n",
       "      <td>0.90920 (k:0.45)</td>\n",
       "      <td>0.72563 (k:0.55)</td>\n",
       "      <td>0.92317 (k:0.73)</td>\n",
       "      <td>0.88043 (k:0.51)</td>\n",
       "      <td>0.56338 (k:0.63)</td>\n",
       "      <td>99.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dynamic:w_augment</th>\n",
       "      <td>0.53388 (k:0.48)</td>\n",
       "      <td>0.83647 (k:0.33)</td>\n",
       "      <td>0.83826 (k:0.72)</td>\n",
       "      <td>0.91506 (k:0.48)</td>\n",
       "      <td>0.90905 (k:0.45)</td>\n",
       "      <td>0.66426 (k:0.40)</td>\n",
       "      <td>0.92202 (k:0.68)</td>\n",
       "      <td>0.88034 (k:0.51)</td>\n",
       "      <td>0.56338 (k:0.61)</td>\n",
       "      <td>98.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dynamic:wo_augment</th>\n",
       "      <td>0.53388 (k:0.48)</td>\n",
       "      <td>0.81559 (k:0.30)</td>\n",
       "      <td>0.74667 (k:0.66)</td>\n",
       "      <td>0.90445 (k:0.41)</td>\n",
       "      <td>0.90893 (k:0.43)</td>\n",
       "      <td>0.70758 (k:0.38)</td>\n",
       "      <td>0.92202 (k:0.69)</td>\n",
       "      <td>0.86572 (k:0.49)</td>\n",
       "      <td>0.56338 (k:0.54)</td>\n",
       "      <td>97.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                cola              mnli              mrpc  \\\n",
       "bert                         0.53388           0.84198           0.84406   \n",
       "dynamic:original    0.53388 (k:0.48)  0.84004 (k:0.33)  0.83942 (k:0.71)   \n",
       "dynamic:w_augment   0.53388 (k:0.48)  0.83647 (k:0.33)  0.83826 (k:0.72)   \n",
       "dynamic:wo_augment  0.53388 (k:0.48)  0.81559 (k:0.30)  0.74667 (k:0.66)   \n",
       "\n",
       "                                qnli               qqp               rte  \\\n",
       "bert                         0.91543           0.90908           0.72563   \n",
       "dynamic:original    0.91287 (k:0.44)  0.90920 (k:0.45)  0.72563 (k:0.55)   \n",
       "dynamic:w_augment   0.91506 (k:0.48)  0.90905 (k:0.45)  0.66426 (k:0.40)   \n",
       "dynamic:wo_augment  0.90445 (k:0.41)  0.90893 (k:0.43)  0.70758 (k:0.38)   \n",
       "\n",
       "                                sst2              stsb              wnli  \\\n",
       "bert                         0.92431           0.88047           0.56338   \n",
       "dynamic:original    0.92317 (k:0.73)  0.88043 (k:0.51)  0.56338 (k:0.63)   \n",
       "dynamic:w_augment   0.92202 (k:0.68)  0.88034 (k:0.51)  0.56338 (k:0.61)   \n",
       "dynamic:wo_augment  0.92202 (k:0.69)  0.86572 (k:0.49)  0.56338 (k:0.54)   \n",
       "\n",
       "                   reproduce  \n",
       "bert                  100.00  \n",
       "dynamic:original       99.87  \n",
       "dynamic:w_augment      98.88  \n",
       "dynamic:wo_augment     97.74  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "with open(PICKLE_PATH, 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "with open('glue_benchmark.pkl', 'rb') as f:\n",
    "    results_original = pickle.load(f)\n",
    "\n",
    "def convert_data_to_results(results):\n",
    "    data = []\n",
    "    subsets = list(results.keys())\n",
    "    factors = list(results[subsets[0]].keys())\n",
    "    for factor in factors:             \n",
    "        row = []\n",
    "        for subset in subsets:\n",
    "            row.append(results[subset][factor])\n",
    "        data.append(row)\n",
    "    return data, factors\n",
    "\n",
    "data, factors = convert_data_to_results(results)\n",
    "data_origin, _ = convert_data_to_results(results_original)\n",
    "data_wo_data_augment = (\"0.53388 (k:0.48)\t0.81559 (k:0.30)\t0.74667 (k:0.66)\t0.90445 (k:0.41)\t\"+\\\n",
    "    \"0.90893 (k:0.43)\t0.70758 (k:0.38)\t0.92202 (k:0.69)\t0.86572 (k:0.49)\t0.56338 (k:0.54)\").split(\"\\t\")\n",
    "\n",
    "factors[1] = \"dynamic:w_augment\"\n",
    "indicies = factors+[\"dynamic:original\", \"dynamic:wo_augment\"]\n",
    "columns = subsets[:]\n",
    "df_data = data + [data_origin[9], data_wo_data_augment]\n",
    "\n",
    "#calculate reproducibility\n",
    "data_scalar = []\n",
    "for line in df_data:\n",
    "    xs = []\n",
    "    for item in line:\n",
    "        xs.append(float(item.split()[0]))\n",
    "    data_scalar.append(xs)\n",
    "reproducibilities = []\n",
    "for i in range(len(data_scalar)):\n",
    "    rsum = 0\n",
    "    for k in range(len(data_scalar[i])):\n",
    "        rsum += data_scalar[i][k]/data_scalar[0][k]\n",
    "    rsum /= len(data_scalar[i])\n",
    "    reproducibilities.append(rsum)\n",
    "for i, r in enumerate(reproducibilities):\n",
    "    df_data[i].append(f\"{r*100:.2f}\")\n",
    "columns.append(\"reproduce\")\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    df_data,\n",
    "    columns=columns, \n",
    "    index=indicies\n",
    ")\n",
    "df = df.reindex([\"bert\", \"dynamic:original\", \"dynamic:w_augment\", \"dynamic:wo_augment\"])\n",
    "tex = df.to_latex()\n",
    "with open(TEX_PATH, 'w') as f:\n",
    "    f.write(tex)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre data augment\n",
    "\n",
    "cola (matthews_correlation)\tmnli (acc)\tmrpc (acc)\tqnli (acc)\tqqp (acc)\trte (acc)\tsst2 (acc)\tstsb (pearson)\twnli (acc)\n",
    "\n",
    "bert\t0.53388\t0.84198\t0.84406\t0.91543\t0.90908\t0.72563\t0.92431\t0.88047\t0.56338\n",
    "\n",
    "dynamic\t0.53388 (k:0.48)\t0.81559 (k:0.30)\t0.74667 (k:0.66)\t0.90445 (k:0.41)\t0.90893 (k:0.43)\t0.70758 (k:0.38)\t0.92202 (k:0.69)\t0.86572 (k:0.49)\t0.56338 (k:0.54)\n",
    "\n",
    "dynamic:original\t0.53388 (k:0.48)\t0.84004 (k:0.33)\t0.83942 (k:0.71)\t0.91287 (k:0.44)\t0.90920 (k:0.45)\t0.72563 (k:0.55)\t0.92317 (k:0.73)\t0.88043 (k:0.51)\t0.56338 (k:0.63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7b3ac0126d0d6fea024471ce24e510948bf6332f7ae1a66cdcb4ee9887514e9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('tensorflow': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
