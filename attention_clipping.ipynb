{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time, random, math, os, sys, tqdm\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from trainer.classification import Trainer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120000/120000 [00:00<00:00, 214283.64it/s]\n",
      "100%|██████████| 7600/7600 [00:00<00:00, 205404.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Dataset Stat.: name:AG_NEWS, nclass:5, max_len:1012, avg_len:236.477525, count:120000\n",
      "Classification Dataset Stat.: name:AG_NEWS, nclass:5, max_len:892, avg_len:235.2992105263158, count:7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-4_H-256_A-4 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer.__init__: Model initialized. model = bert\n",
      "Trainer.load: Loading...\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(batch_size=8)\n",
    "trainer.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = trainer.get_batch()\n",
    "trainer.model = trainer.model.eval()\n",
    "bert = trainer.model.bert\n",
    "fc = trainer.model.classifier\n",
    "\n",
    "def eval(model, batch=batch, fc=fc):\n",
    "    lm_output = model(\n",
    "        input_ids = batch.input_ids, \n",
    "        attention_mask = batch.attention_masks, \n",
    "        output_hidden_states=True,\n",
    "        output_attentions=True,\n",
    "    )\n",
    "    last_hidden = lm_output.last_hidden_state[:,0,:]\n",
    "    x = fc(last_hidden)\n",
    "    return torch.argmax(x, dim=-1), batch.labels, lm_output\n",
    "output, labels, lm_output = eval(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 34, torch.Size([8, 4, 71, 71]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LAYER = -1\n",
    "BATCH = 0\n",
    "HEAD = 0\n",
    "N = torch.sum(batch.attention_masks[BATCH]).item()\n",
    "\n",
    "plt.imshow(lm_output.attentions[LAYER][BATCH][HEAD][:N, :N].detach().cpu().numpy())\n",
    "plt.show()\n",
    "\n",
    "torch.sum(lm_output.attentions[LAYER][BATCH][HEAD][0,:N]).item(), N, lm_output.attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "impact factor torch.Size([34])\n",
      "hign impact [33  0 27 18  7]\n",
      "0.01699972152709961\n",
      "tensor([33,  0, 27, 18,  7, 17, 12, 29, 24, 32], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, 27, 33, 14, 31, 21, 16,  6, 18, 29,  8, 25,  5,  7, 32, 13, 28, 17,\n",
      "         1,  4], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "NBATCH = batch.input_ids.shape[0]\n",
    "LAYER = 3\n",
    "attend_tokens = [0]\n",
    "\n",
    "a = lm_output.attentions[LAYER][BATCH][:,:N, :N]\n",
    "a = torch.sum(a, dim=0)\n",
    "a = a[attend_tokens,:]\n",
    "a = torch.sum(a, dim=0)\n",
    "print('impact factor', a.shape)\n",
    "b = torch.topk(a, 5)[1]\n",
    "print('hign impact', b.cpu().numpy())\n",
    "# plt.bar(range(N), a.detach().cpu().numpy())\n",
    "# plt.show()\n",
    "\n",
    "#this is very evil code...\n",
    "def update_input_mask_from_previous_attention(attention_mask, previous_attention, output_token_indices, output_token_impact, k=5):\n",
    "    NBATCH = attention_mask.shape[0]\n",
    "\n",
    "    input_indcies = []\n",
    "    input_impacts = []\n",
    "    for i, idxs in enumerate(output_token_indices):\n",
    "        N = int(torch.sum((attention_mask[i]==0)*1.0).item())\n",
    "        \n",
    "        #print(i, previous_attention, N)\n",
    "        a = previous_attention[i,:,:N,:N]\n",
    "        #print(a.shape, N, previous_attention.shape)\n",
    "        a = torch.sum(a, dim=0)\n",
    "        #print(idxs, a.shape)\n",
    "        a = a[idxs, :] * output_token_impact[i].view(-1, 1)\n",
    "        a = torch.sum(a, dim=0)\n",
    "        \n",
    "        #print(f'k {k}, a {a.shape}')\n",
    "        kxx = k\n",
    "        if k < 1.0: kxx = int(math.ceil(k*N))\n",
    "        b, c = torch.topk(a, min(N, kxx))\n",
    "        input_indcies.append(c)\n",
    "        input_impacts.append(b)\n",
    "    \n",
    "    input_mask = torch.zeros(NBATCH, attention_mask.shape[-1], device=previous_attention.device, dtype=previous_attention.dtype)\n",
    "    for i, idxs in enumerate(input_indcies):\n",
    "        for k in idxs:\n",
    "            input_mask[i, k] = 1.0\n",
    "    \n",
    "    return input_mask, input_indcies, input_impacts\n",
    "\n",
    "t = time.time()\n",
    "mask, indices, impacts = update_input_mask_from_previous_attention(\n",
    "    batch.attention_masks - 1.0, \n",
    "    lm_output.attentions[LAYER], \n",
    "    [[0]] * NBATCH, \n",
    "    torch.ones(NBATCH, 1, device=batch.device, dtype=torch.float32), \n",
    "    k=10\n",
    ")\n",
    "mask2, indices2, impacts2 = update_input_mask_from_previous_attention(\n",
    "    batch.attention_masks - 1.0, \n",
    "    lm_output.attentions[LAYER-1], \n",
    "    indices, \n",
    "    impacts, \n",
    "    k=20\n",
    ")\n",
    "print(time.time() - t)\n",
    "mask2.shape, batch.attention_masks.shape\n",
    "def plot_grid(grid):\n",
    "    plt.imshow(grid.cpu().detach().numpy())\n",
    "    #plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "print(indices[0])\n",
    "last_output_attentions = torch.sum(lm_output.attentions[LAYER][0], dim=0)[[0], :N]\n",
    "plot_grid(last_output_attentions)\n",
    "last_output_attentions_input_masked = last_output_attentions * mask[0][:N]\n",
    "plot_grid(last_output_attentions_input_masked)\n",
    "\n",
    "print(indices2[0])\n",
    "second_output_attentions = torch.sum(lm_output.attentions[LAYER-1][0], dim=0)[indices[0],:N]*impacts[0].view(-1,1)\n",
    "plot_grid(second_output_attentions)\n",
    "second_output_attentions_impact = torch.sum(second_output_attentions, dim=0).view(1,-1)\n",
    "plot_grid(second_output_attentions_impact)\n",
    "plot_grid(mask2[0][:N].view(1,-1))\n",
    "plot_grid(second_output_attentions_impact * mask2[0][:N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, os, torch\n",
    "import torch.utils.checkpoint\n",
    "from packaging import version\n",
    "from torch import nn\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    BaseModelOutputWithPoolingAndCrossAttentions,\n",
    ")\n",
    "from transformers.modeling_utils import (\n",
    "    PreTrainedModel,\n",
    "    apply_chunking_to_forward,\n",
    "    find_pruneable_heads_and_indices,\n",
    "    prune_linear_layer,\n",
    ")\n",
    "from transformers.utils import logging\n",
    "from transformers.models.bert.configuration_bert import BertConfig\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "from transformers.models.bert.modeling_bert import BertEmbeddings\n",
    "\n",
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config, position_embedding_type=None):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
    "                f\"heads ({config.num_attention_heads})\"\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.position_embedding_type = position_embedding_type or getattr(\n",
    "            config, \"position_embedding_type\", \"absolute\"\n",
    "        )\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            raise Exception('removed')\n",
    "\n",
    "        self.is_decoder = config.is_decoder\n",
    "        if self.is_decoder: raise Exception()\n",
    "\n",
    "        self.print = True\n",
    "        self.reset_input_mask()\n",
    "        self.attention_masking_timing = 'after_softmax'\n",
    "        self.output_masking = True\n",
    "    \n",
    "    def reset_input_mask(self):\n",
    "        self.input_mask = None\n",
    "        self.input_indices = None\n",
    "        self.input_impacts = None\n",
    "        self.output_mask = None\n",
    "    \n",
    "    def update_input_mask_from_previous_attention(self, output_token_mask, output_token_indices, output_token_impact, k):\n",
    "        input_mask, input_indcies, input_impacts = update_input_mask_from_previous_attention(\n",
    "            self.last_attention_mask, \n",
    "            self.last_attention_probs, \n",
    "            output_token_indices, \n",
    "            output_token_impact, \n",
    "            k\n",
    "        )\n",
    "        self.input_mask = input_mask\n",
    "        self.input_indices = input_indcies\n",
    "        self.input_impacts = input_impacts\n",
    "        self.output_mask = output_token_mask\n",
    "        return input_mask, input_indcies, input_impacts\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        if self.is_decoder: raise Exception()\n",
    "        if not self.attention_masking_timing in ['after_softmax', 'before_softmax']: raise Exception()\n",
    "        self.last_hidden_states = hidden_states\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        \n",
    "        is_cross_attention = encoder_hidden_states is not None\n",
    "\n",
    "        if (is_cross_attention and past_key_value is not None) or is_cross_attention or (past_key_value is not None):\n",
    "            raise Exception()\n",
    "        else:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            raise Exception()\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        if self.input_mask is not None and self.attention_masking_timing == 'before_softmax':\n",
    "            if self.print: print(f'apply input mask, before softmax. input_mask:{self.input_mask.shape}, attention_scores:{attention_scores.shape}')\n",
    "            attention_scores = attention_scores + (1.0 - self.input_mask.view(self.input_mask.shape[0], 1, 1, self.input_mask.shape[-1])) * -10000\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "        \n",
    "        if self.print: print('SelfAttention.forward: last_attention_probs backuped')\n",
    "        self.last_attention_probs = attention_probs.detach().clone()\n",
    "        self.last_attention_mask = attention_mask.detach().clone()\n",
    "\n",
    "        if self.input_mask is not None and self.attention_masking_timing == 'after_softmax':\n",
    "            if self.print: print(f'apply input mask, after softmax. input_mask:{self.input_mask.shape}, attention_probs:{attention_probs.shape}')\n",
    "            attention_probs = attention_probs * self.input_mask.view(self.input_mask.shape[0], 1, 1, self.input_mask.shape[-1])\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        if self.output_mask is not None and self.output_masking:\n",
    "            output_mask = self.output_mask.unsqueeze(-1)\n",
    "            if self.print: print(f'apply output mask. mask:{output_mask.shape} context:{context_layer.shape}')\n",
    "            context_layer = context_layer * output_mask\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, config, position_embedding_type=None):\n",
    "        super().__init__()\n",
    "        self.self = BertSelfAttention(config, position_embedding_type=position_embedding_type)\n",
    "        self.output = BertSelfOutput(config)\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        heads, index = find_pruneable_heads_and_indices(\n",
    "            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
    "        )\n",
    "\n",
    "        # Prune linear layers\n",
    "        self.self.query = prune_linear_layer(self.self.query, index)\n",
    "        self.self.key = prune_linear_layer(self.self.key, index)\n",
    "        self.self.value = prune_linear_layer(self.self.value, index)\n",
    "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
    "\n",
    "        # Update hyper params and store pruned heads\n",
    "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
    "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        self_outputs = self.self(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            encoder_hidden_states,\n",
    "            encoder_attention_mask,\n",
    "            past_key_value,\n",
    "            output_attentions,\n",
    "        )\n",
    "        attention_output = self.output(self_outputs[0], hidden_states)\n",
    "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.attention = BertAttention(config)\n",
    "        self.is_decoder = config.is_decoder\n",
    "        self.add_cross_attention = config.add_cross_attention\n",
    "        if self.add_cross_attention:\n",
    "            if not self.is_decoder:\n",
    "                raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n",
    "            self.crossattention = BertAttention(config, position_embedding_type=\"absolute\")\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
    "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
    "        self_attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            past_key_value=self_attn_past_key_value,\n",
    "        )\n",
    "        self.self_attention_outputs = self_attention_outputs\n",
    "        attention_output = self_attention_outputs[0]\n",
    "\n",
    "        # if decoder, the last output is tuple of self-attn cache\n",
    "        if self.is_decoder: raise Exception()\n",
    "        else: outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        cross_attn_present_key_value = None\n",
    "        if self.is_decoder and encoder_hidden_states is not None: raise Exception()\n",
    "\n",
    "        layer_output = apply_chunking_to_forward(\n",
    "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
    "        )\n",
    "        self.layer_output = layer_output\n",
    "        outputs = (layer_output,) + outputs\n",
    "\n",
    "        # if decoder, return the attn key/values as the last output\n",
    "        if self.is_decoder: raise Exception()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def feed_forward_chunk(self, attention_output):\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output\n",
    "\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                if use_cache:\n",
    "                    logger.warning(\n",
    "                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                    )\n",
    "                    use_cache = False\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(layer_module),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    past_key_value,\n",
    "                    output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[-1],)\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "\n",
    "class BertPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "\n",
    "    config_class = BertConfig\n",
    "    #load_tf_weights = load_tf_weights_in_bert\n",
    "    base_model_prefix = \"bert\"\n",
    "    supports_gradient_checkpointing = True\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def _set_gradient_checkpointing(self, module, value=False):\n",
    "        if isinstance(module, BertEncoder):\n",
    "            module.gradient_checkpointing = value\n",
    "\n",
    "\n",
    "class BertModel(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "\n",
    "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
    "    cross-attention is added between the self-attention layers, following the architecture described in [Attention is\n",
    "    all you need](https://arxiv.org/abs/1706.03762) by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n",
    "    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n",
    "\n",
    "    To behave as an decoder the model needs to be initialized with the `is_decoder` argument of the configuration set\n",
    "    to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n",
    "    `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, add_pooling_layer=True):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "\n",
    "        self.pooler = BertPooler(config) if add_pooling_layer else None\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\"\n",
    "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
    "        class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "            \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
    "            the model is configured as a decoder.\n",
    "        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
    "            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
    "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
    "\n",
    "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
    "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
    "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
    "        use_cache (`bool`, *optional*):\n",
    "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
    "            `past_key_values`).\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if self.config.is_decoder:\n",
    "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        else:\n",
    "            use_cache = False\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        batch_size, seq_length = input_shape\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        # past_key_values_length\n",
    "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            if hasattr(self.embeddings, \"token_type_ids\"):\n",
    "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
    "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
    "                token_type_ids = buffered_token_type_ids_expanded\n",
    "            else:\n",
    "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
    "\n",
    "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            past_key_values_length=past_key_values_length,\n",
    "        )\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            past_key_values=encoder_outputs.past_key_values,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            cross_attentions=encoder_outputs.cross_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttention.forward: last_attention_probs backuped\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "SelfAttention.forward: last_attention_probs backuped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 4, 1, 3, 4, 3, 3], device='cuda:0'),\n",
       " tensor([1, 2, 4, 1, 3, 4, 3, 3], device='cuda:0'))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsebert = BertModel(trainer.model.bert.config)\n",
    "sparsebert = sparsebert.to(trainer.device)\n",
    "sparsebert = sparsebert.eval()\n",
    "sparsebert.load_state_dict(bert.state_dict())\n",
    "eval(sparsebert)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttention.forward: last_attention_probs backuped\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "apply input mask, before softmax. input_mask:torch.Size([8, 71]), attention_scores:torch.Size([8, 4, 71, 71])\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "apply input mask, before softmax. input_mask:torch.Size([8, 71]), attention_scores:torch.Size([8, 4, 71, 71])\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "apply input mask, before softmax. input_mask:torch.Size([8, 71]), attention_scores:torch.Size([8, 4, 71, 71])\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "apply input mask, before softmax. input_mask:torch.Size([8, 71]), attention_scores:torch.Size([8, 4, 71, 71])\n",
      "SelfAttention.forward: last_attention_probs backuped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 4, 1, 3, 4, 3, 3], device='cuda:0'),\n",
       " tensor([1, 2, 4, 1, 3, 4, 3, 3], device='cuda:0'))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def set_print(sparsebert, v):\n",
    "    for layer in sparsebert.encoder.layer:\n",
    "        layer.attention.self.print = v\n",
    "\n",
    "def set_masking_timing(sparsebert, v):\n",
    "    for layer in sparsebert.encoder.layer:\n",
    "        layer.attention.self.attention_masking_timing = v\n",
    "\n",
    "def set_output_masking(sparsebert, v):\n",
    "    for layer in sparsebert.encoder.layer:\n",
    "        layer.attention.self.output_masking = v\n",
    "\n",
    "def update_input_mask(sparsebert, ks=[0.999,0.5,0.25,0.1]):\n",
    "    dtype = sparsebert.encoder.layer[0].attention.self.last_attention_probs.dtype\n",
    "    device = sparsebert.encoder.layer[0].attention.self.last_attention_probs.device\n",
    "    batch_size = sparsebert.encoder.layer[0].attention.self.last_attention_mask.shape[0]\n",
    "    token_len = sparsebert.encoder.layer[0].attention.self.last_attention_mask.shape[-1]\n",
    "    mask = torch.zeros(batch_size, token_len, dtype=dtype, device=device)\n",
    "    mask[:,0] = 1.0\n",
    "    indices = torch.zeros(batch_size, 1, dtype=torch.int64, device=device)\n",
    "    impacts = torch.ones(batch_size, 1, dtype=dtype, device=device)\n",
    "    L = len(sparsebert.encoder.layer)\n",
    "    for i in range(L):\n",
    "        mask, indices, impacts = sparsebert.encoder.layer[L-i-1].attention.self.update_input_mask_from_previous_attention(\n",
    "            output_token_mask = mask,\n",
    "            output_token_indices = indices,\n",
    "            output_token_impact = impacts,\n",
    "            k = ks[L-i-1],\n",
    "        )\n",
    "\n",
    "def reset_input_mask(sparsebert):\n",
    "    for layer in sparsebert.encoder.layer:\n",
    "        layer.attention.self.reset_input_mask()\n",
    "\n",
    "set_print(sparsebert, True)\n",
    "set_output_masking(sparsebert, False)\n",
    "set_masking_timing(sparsebert, 'before_softmax')\n",
    "reset_input_mask(sparsebert)\n",
    "eval(sparsebert)\n",
    "update_input_mask(sparsebert)\n",
    "ret = eval(sparsebert)[:2]\n",
    "reset_input_mask(sparsebert)\n",
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:09<00:00, 103.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.93475"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(bert, eval=eval, N=1000):\n",
    "    trainer.seed()\n",
    "    acc_sum = 0\n",
    "    N = N\n",
    "    for i in tqdm.tqdm(range(N)):\n",
    "        batch = trainer.get_batch(test=True)\n",
    "        output, label, _ = eval(bert, batch=batch)\n",
    "        acc_sum += torch.mean((output == label) * 1.0)\n",
    "    return acc_sum.item() / N\n",
    "accuracy(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:04<00:00, 15.58it/s]\n",
      "100%|██████████| 1000/1000 [01:03<00:00, 15.68it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('output mask: False', 0.92425, 0.9275)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sparse_eval(sparsebert, batch=batch):\n",
    "    reset_input_mask(sparsebert)\n",
    "    eval(sparsebert, batch=batch)\n",
    "    update_input_mask(sparsebert)\n",
    "    ret = eval(sparsebert, batch=batch)\n",
    "    return ret\n",
    "set_output_masking(sparsebert, False)\n",
    "set_masking_timing(sparsebert, 'after_softmax')\n",
    "set_print(sparsebert, False)\n",
    "acc_after = accuracy(sparsebert, eval=sparse_eval)\n",
    "set_print(sparsebert, True)\n",
    "set_masking_timing(sparsebert, 'before_softmax')\n",
    "set_print(sparsebert, False)\n",
    "acc_before = accuracy(sparsebert, eval=sparse_eval)\n",
    "set_print(sparsebert, True)\n",
    "'output mask: False', acc_after, acc_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:03<00:00, 15.74it/s]\n",
      "100%|██████████| 1000/1000 [01:04<00:00, 15.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('output mask: True', 0.915625, 0.924625)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_output_masking(sparsebert, True)\n",
    "set_masking_timing(sparsebert, 'after_softmax') #this must be wrong.\n",
    "set_print(sparsebert, False)\n",
    "acc_after = accuracy(sparsebert, eval=sparse_eval)\n",
    "set_print(sparsebert, True)\n",
    "set_masking_timing(sparsebert, 'before_softmax')\n",
    "set_print(sparsebert, False)\n",
    "acc_before = accuracy(sparsebert, eval=sparse_eval)\n",
    "set_print(sparsebert, True)\n",
    "'output mask: True', acc_after, acc_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention MSE, after softmax tensor(0.0025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Attention MSE, befo softmax tensor(0.0031, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "set_print(sparsebert, False)\n",
    "set_masking_timing(sparsebert, 'after_softmax')\n",
    "reset_input_mask(sparsebert)\n",
    "_,_, lm_output = eval(sparsebert, batch=batch)\n",
    "update_input_mask(sparsebert)\n",
    "_,_, lm_output_sparse = eval(sparsebert, batch=batch)\n",
    "reset_input_mask(sparsebert)\n",
    "print('Attention MSE, after softmax', torch.mean(torch.square(lm_output.attentions[-1]-lm_output_sparse.attentions[-1])))\n",
    "\n",
    "set_masking_timing(sparsebert, 'before_softmax')\n",
    "reset_input_mask(sparsebert)\n",
    "_,_, lm_output = eval(sparsebert, batch=batch)\n",
    "update_input_mask(sparsebert)\n",
    "_,_, lm_output_sparse = eval(sparsebert, batch=batch)\n",
    "reset_input_mask(sparsebert)\n",
    "print('Attention MSE, befo softmax', torch.mean(torch.square(lm_output.attentions[-1]-lm_output_sparse.attentions[-1])))\n",
    "set_print(sparsebert, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure input and output mask is same\n",
    "# set_output_masking(sparsebert, True)\n",
    "# reset_input_mask(sparsebert)\n",
    "# eval(sparsebert, batch=batch)\n",
    "# update_input_mask(sparsebert)\n",
    "# ret = eval(sparsebert, batch=batch)\n",
    "# for i in range(3):\n",
    "#     a = sparsebert.encoder.layer[i].attention.self.output_mask\n",
    "#     b = sparsebert.encoder.layer[i+1].attention.self.input_mask\n",
    "#     print(torch.mean(torch.abs(a-b)))\n",
    "# reset_input_mask(sparsebert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3052, -0.0182,  0.4914, -0.5295,  1.0349],\n",
      "        [ 0.1508, -0.2283, -0.1500, -0.2335,  0.7907],\n",
      "        [-0.6254, -0.0668, -0.5428, -0.7241,  1.5681],\n",
      "        [ 0.4267, -0.0869, -0.0414,  0.4975, -1.6607]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.4067, -0.2589,  0.5076, -0.1107,  0.9590],\n",
      "        [-0.1066, -0.5086,  0.3520,  0.1056,  0.3538],\n",
      "        [-1.0309, -0.5523, -0.5106, -0.6852,  2.1186],\n",
      "        [ 0.5187, -0.1235,  0.3521,  1.6831, -1.3839]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([1., 0., 0., 1.], device='cuda:0')\n",
      "tensor([1., 0., 0., 1.], device='cuda:0')\n",
      "tensor([[-0.0822, -0.4312,  0.5096,  0.0294,  1.0529],\n",
      "        [-0.3884, -0.4388,  0.1020,  0.4166,  0.2858],\n",
      "        [-0.8628, -0.5308, -0.6314, -0.2096,  1.2180],\n",
      "        [ 1.0807, -0.3690,  0.1572,  0.9194, -0.5309]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.4568, -0.4038,  0.3431, -0.0683,  0.7169],\n",
      "        [-0.0599, -0.3666,  0.0750,  0.3621, -0.2516],\n",
      "        [-0.3265, -0.7215, -0.8256, -0.4296,  1.4059],\n",
      "        [ 1.9299, -0.0835, -0.6393,  0.4410, -1.0698]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.3052, -0.0182,  0.4914, -0.5295,  1.0349],\n",
      "        [-0.5102,  0.0986,  0.1125, -0.0224,  0.8017],\n",
      "        [-1.0530,  0.4784, -0.7741, -0.4987,  1.3473],\n",
      "        [ 0.4267, -0.0869, -0.0414,  0.4975, -1.6607]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.4067, -0.2589,  0.5076, -0.1107,  0.9590],\n",
      "        [-0.2307, -0.1605,  0.5216,  0.0949,  0.1396],\n",
      "        [-1.2267,  0.4728, -0.5494, -0.6557,  1.7330],\n",
      "        [ 0.5187, -0.1235,  0.3521,  1.6831, -1.3839]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([1., 0., 0., 1.], device='cuda:0')\n",
      "tensor([1., 0., 0., 1.], device='cuda:0')\n",
      "tensor([[-0.0822, -0.4312,  0.5096,  0.0294,  1.0529],\n",
      "        [-0.2188,  0.1564,  0.3835,  0.0084,  0.1328],\n",
      "        [-0.9983,  0.7221, -0.5428, -0.5966,  1.4275],\n",
      "        [ 0.4055,  0.1916,  0.2055,  1.2778, -1.1371]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.4568, -0.4038,  0.3431, -0.0683,  0.7169],\n",
      "        [-0.0630,  0.0976,  0.2362,  0.2903,  0.1388],\n",
      "        [-0.5496,  0.8150, -0.6720, -0.8740,  1.3315],\n",
      "        [ 0.6600,  0.1397, -0.2790,  1.0046, -1.7808]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([1., 0., 0., 0.], device='cuda:0')\n",
      "tensor([1., 0., 0., 0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "set_print(sparsebert, False)\n",
    "set_masking_timing(sparsebert, 'before_softmax')\n",
    "set_output_masking(sparsebert, False)\n",
    "a = sparse_eval(sparsebert, batch=batch)[-1].hidden_states\n",
    "print(sparsebert.encoder.layer[0].self_attention_outputs[0][0][:4,:5])\n",
    "print(sparsebert.encoder.layer[1].attention.self.last_hidden_states[0,:4,:5])\n",
    "print(sparsebert.encoder.layer[0].attention.self.output_mask[0,:4])\n",
    "print(sparsebert.encoder.layer[1].attention.self.input_mask[0,:4])\n",
    "print(sparsebert.encoder.layer[1].self_attention_outputs[0][0][:4,:5])\n",
    "print(sparsebert.encoder.layer[2].attention.self.last_hidden_states[0,:4,:5])\n",
    "set_output_masking(sparsebert, True)\n",
    "b = sparse_eval(sparsebert, batch=batch)[-1].hidden_states\n",
    "b[-1].shape, torch.mean(torch.abs(a[-1]-b[-1]))\n",
    "b[-1][0][1] # even if i mask output, the result is not 0 because of layernorm? (I am not sure)\n",
    "print(sparsebert.encoder.layer[0].self_attention_outputs[0][0][:4,:5])\n",
    "print(sparsebert.encoder.layer[1].attention.self.last_hidden_states[0,:4,:5])\n",
    "print(sparsebert.encoder.layer[0].attention.self.output_mask[0,:4])\n",
    "print(sparsebert.encoder.layer[1].attention.self.input_mask[0,:4])\n",
    "print(sparsebert.encoder.layer[1].self_attention_outputs[0][0][:4,:5])\n",
    "print(sparsebert.encoder.layer[2].attention.self.last_hidden_states[0,:4,:5])\n",
    "print(sparsebert.encoder.layer[1].attention.self.output_mask[0,:4])\n",
    "print(sparsebert.encoder.layer[2].attention.self.input_mask[0,:4])\n",
    "#sparsebert.encoder.layer[-1].attention.self.output_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approx attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120000/120000 [00:00<00:00, 216214.05it/s]\n",
      "100%|██████████| 7600/7600 [00:00<00:00, 217140.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Dataset Stat.: name:AG_NEWS, nclass:5, max_len:1012, avg_len:236.477525, count:120000\n",
      "Classification Dataset Stat.: name:AG_NEWS, nclass:5, max_len:892, avg_len:235.2992105263158, count:7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-4_H-256_A-4 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer.__init__: Model initialized. model = bert\n",
      "Trainer.load: Loading...\n"
     ]
    }
   ],
   "source": [
    "from trainer.attention_approx import Trainer as ApproxTrainer\n",
    "approx_trainer = ApproxTrainer(factor=16)\n",
    "approx_trainer.load()\n",
    "approxbert = approx_trainer.bert\n",
    "tbatch = trainer.get_batch(test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L = -1\n",
    "plot_grid(eval(bert, batch=tbatch)[-1].attentions[L][0,1])\n",
    "plot_grid(eval(approxbert, batch=tbatch, fc=lambda x:x)[-1].attentions[L][0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 736/1500 [00:56<00:59, 12.93it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-160-1c37c633659e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mset_masking_timing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparsebert\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'before_softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mset_output_masking\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparsebert\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparsebert\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mapprox_eval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-137-3627c762934d>\u001b[0m in \u001b[0;36maccuracy\u001b[1;34m(bert, eval, N)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0macc_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0macc_sum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-160-1c37c633659e>\u001b[0m in \u001b[0;36mapprox_eval\u001b[1;34m(sparsebert, batch, approxbert, ks)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_attention_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattentions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mupdate_input_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparsebert\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparsebert\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#approx_eval(sparsebert)[:2]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-e7aab8cc3bfd>\u001b[0m in \u001b[0;36meval\u001b[1;34m(model, batch, fc)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     lm_output = model(\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention_masks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-96-4a66d58b46b9>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    590\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m         )\n\u001b[1;32m--> 592\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    593\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-96-4a66d58b46b9>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    359\u001b[0m                 )\n\u001b[0;32m    360\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 361\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    362\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-96-4a66d58b46b9>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[1;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[0;32m    273\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-96-4a66d58b46b9>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         )\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# add attentions if we output them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-96-4a66d58b46b9>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\normalization.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         return F.layer_norm(\n\u001b[0m\u001b[0;32m    190\u001b[0m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2345\u001b[0m             \u001b[0mlayer_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2346\u001b[0m         )\n\u001b[1;32m-> 2347\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def approx_eval(sparsebert, batch=batch, approxbert=approxbert, ks=[0.5,0.5,0.5,0.5]):\n",
    "    attentions = eval(approxbert, batch=batch, fc=lambda x:x)[-1].attentions\n",
    "    reset_input_mask(sparsebert)\n",
    "    eval(sparsebert, batch=batch)\n",
    "    for i, layer in enumerate(sparsebert.encoder.layer):\n",
    "        assert layer.attention.self.last_attention_probs.shape == attentions[i].shape\n",
    "        layer.attention.self.last_attention_probs = attentions[i]\n",
    "    update_input_mask(sparsebert, ks=ks)\n",
    "    ret = eval(sparsebert, batch=batch)\n",
    "    return ret\n",
    "#approx_eval(sparsebert)[:2]\n",
    "set_print(sparsebert, False)\n",
    "set_masking_timing(sparsebert, 'before_softmax')\n",
    "set_output_masking(sparsebert, True)\n",
    "accuracy(sparsebert, eval=approx_eval, N=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(17., device='cuda:0'), tensor(34, device='cuda:0'))"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = approx_eval(sparsebert)[-1]\n",
    "torch.sum((lm.attentions[0][0,0][0] > 0.0) * 1.0), torch.sum(batch.attention_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7b3ac0126d0d6fea024471ce24e510948bf6332f7ae1a66cdcb4ee9887514e9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('tensorflow': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
