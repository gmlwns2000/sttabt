{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time, random, math, os, sys\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from trainer.classification import Trainer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120000/120000 [00:00<00:00, 212014.23it/s]\n",
      "100%|██████████| 7600/7600 [00:00<00:00, 211112.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Dataset Stat.: name:AG_NEWS, nclass:5, max_len:1012, avg_len:236.477525, count:120000\n",
      "Classification Dataset Stat.: name:AG_NEWS, nclass:5, max_len:892, avg_len:235.2992105263158, count:7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-4_H-256_A-4 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer.__init__: Model initialized. model = bert\n",
      "Trainer.load: Loading...\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(batch_size=32)\n",
    "trainer.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = trainer.get_batch()\n",
    "trainer.model = trainer.model.eval()\n",
    "bert = trainer.model.bert\n",
    "fc = trainer.model.classifier\n",
    "\n",
    "def eval(model):\n",
    "    lm_output = model(\n",
    "        input_ids = batch.input_ids, \n",
    "        attention_mask = batch.attention_masks, \n",
    "        output_hidden_states=True,\n",
    "        output_attentions=True,\n",
    "    )\n",
    "    last_hidden = lm_output.last_hidden_state[:,0,:]\n",
    "    x = fc(last_hidden)\n",
    "    return torch.argmax(x, dim=-1), batch.labels, lm_output\n",
    "output, labels, lm_output = eval(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1.0000001192092896, 60, torch.Size([32, 4, 120, 120]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LAYER = -1\n",
    "BATCH = 0\n",
    "HEAD = 0\n",
    "N = torch.sum(batch.attention_masks[BATCH]).item()\n",
    "\n",
    "plt.imshow(lm_output.attentions[LAYER][BATCH][HEAD][:N, :N].detach().cpu().numpy())\n",
    "plt.show()\n",
    "\n",
    "torch.sum(lm_output.attentions[LAYER][BATCH][HEAD][0,:N]).item(), N, lm_output.attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "impact factor torch.Size([60])\n",
      "hign impact [15 18  3 13 51]\n",
      "0.07299947738647461\n",
      "tensor([15, 18,  3, 13, 51, 17, 44, 41, 19, 20], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, 14, 19, 18, 43, 17, 50, 16, 40, 24, 59, 21,  3, 27, 26, 44, 52, 30,\n",
      "        23, 12], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "NBATCH = batch.input_ids.shape[0]\n",
    "LAYER = 3\n",
    "attend_tokens = [0]\n",
    "\n",
    "a = lm_output.attentions[LAYER][BATCH][:,:N, :N]\n",
    "a = torch.sum(a, dim=0)\n",
    "a = a[attend_tokens,:]\n",
    "a = torch.sum(a, dim=0)\n",
    "print('impact factor', a.shape)\n",
    "b = torch.topk(a, 5)[1]\n",
    "print('hign impact', b.cpu().numpy())\n",
    "# plt.bar(range(N), a.detach().cpu().numpy())\n",
    "# plt.show()\n",
    "\n",
    "#this is very evil code...\n",
    "def update_input_mask_from_previous_attention(attention_mask, previous_attention, output_token_indcies, output_token_impact, k=5):\n",
    "    NBATCH = len(output_token_indcies)\n",
    "\n",
    "    input_indcies = []\n",
    "    input_impacts = []\n",
    "    for i, idxs in enumerate(output_token_indcies):\n",
    "        N = torch.sum(attention_mask[i])\n",
    "        \n",
    "        a = previous_attention[i,:,:N,:N]\n",
    "        a = torch.sum(a, dim=0)\n",
    "        a = a[idxs, :] * output_token_impact[i].view(-1, 1)\n",
    "        a = torch.sum(a, dim=0)\n",
    "        \n",
    "        b, c = torch.topk(a, k)\n",
    "        input_indcies.append(c)\n",
    "        input_impacts.append(b)\n",
    "    \n",
    "    input_mask = torch.zeros(NBATCH, attention_mask.shape[1], device=previous_attention.device)#, dtype=previous_attention.dtype)\n",
    "    for i, idxs in enumerate(input_indcies):\n",
    "        for k in idxs:\n",
    "            input_mask[i, k] = 1.0\n",
    "    \n",
    "    return input_mask, input_indcies, input_impacts\n",
    "\n",
    "t = time.time()\n",
    "mask, indices, impacts = update_input_mask_from_previous_attention(\n",
    "    batch.attention_masks, \n",
    "    lm_output.attentions[LAYER], \n",
    "    [[0]] * NBATCH, \n",
    "    torch.ones(NBATCH, 1, device=batch.device, dtype=torch.float32), \n",
    "    k=10\n",
    ")\n",
    "mask2, indices2, impacts2 = update_input_mask_from_previous_attention(\n",
    "    batch.attention_masks, \n",
    "    lm_output.attentions[LAYER-1], \n",
    "    indices, \n",
    "    impacts, \n",
    "    k=20\n",
    ")\n",
    "print(time.time() - t)\n",
    "mask2.shape, batch.attention_masks.shape\n",
    "def plot_grid(grid):\n",
    "    plt.imshow(grid.cpu().detach().numpy())\n",
    "    #plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "print(indices[0])\n",
    "last_output_attentions = torch.sum(lm_output.attentions[LAYER][0], dim=0)[[0], :N]\n",
    "plot_grid(last_output_attentions)\n",
    "last_output_attentions_input_masked = last_output_attentions * mask[0][:N]\n",
    "plot_grid(last_output_attentions_input_masked)\n",
    "\n",
    "print(indices2[0])\n",
    "second_output_attentions = torch.sum(lm_output.attentions[LAYER-1][0], dim=0)[indices[0],:N]*impacts[0].view(-1,1)\n",
    "plot_grid(second_output_attentions)\n",
    "second_output_attentions_impact = torch.sum(second_output_attentions, dim=0).view(1,-1)\n",
    "plot_grid(second_output_attentions_impact)\n",
    "plot_grid(mask2[0][:N].view(1,-1))\n",
    "plot_grid(second_output_attentions_impact * mask2[0][:N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, os, torch\n",
    "import torch.utils.checkpoint\n",
    "from packaging import version\n",
    "from torch import nn\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.file_utils import (\n",
    "    add_code_sample_docstrings,\\\n",
    "    add_start_docstrings_to_model_forward,\n",
    ")\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    BaseModelOutputWithPoolingAndCrossAttentions,\n",
    ")\n",
    "from transformers.modeling_utils import (\n",
    "    PreTrainedModel,\n",
    "    apply_chunking_to_forward,\n",
    "    find_pruneable_heads_and_indices,\n",
    "    prune_linear_layer,\n",
    ")\n",
    "from transformers.utils import logging\n",
    "from transformers.models.bert.configuration_bert import BertConfig\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "        if version.parse(torch.__version__) > version.parse(\"1.6.0\"):\n",
    "            self.register_buffer(\n",
    "                \"token_type_ids\",\n",
    "                torch.zeros(self.position_ids.size(), dtype=torch.long),\n",
    "                persistent=False,\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n",
    "    ):\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        else:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n",
    "\n",
    "        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n",
    "        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n",
    "        # issue #5664\n",
    "        if token_type_ids is None:\n",
    "            if hasattr(self, \"token_type_ids\"):\n",
    "                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n",
    "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n",
    "                token_type_ids = buffered_token_type_ids_expanded\n",
    "            else:\n",
    "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.word_embeddings(input_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = inputs_embeds + token_type_embeddings\n",
    "        if self.position_embedding_type == \"absolute\":\n",
    "            position_embeddings = self.position_embeddings(position_ids)\n",
    "            embeddings += position_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config, position_embedding_type=None):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
    "                f\"heads ({config.num_attention_heads})\"\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.position_embedding_type = position_embedding_type or getattr(\n",
    "            config, \"position_embedding_type\", \"absolute\"\n",
    "        )\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            raise Exception('removed')\n",
    "\n",
    "        self.is_decoder = config.is_decoder\n",
    "        if self.is_decoder: raise Exception()\n",
    "\n",
    "        self.print = True\n",
    "        self.input_mask = None\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        if self.is_decoder: raise Exception()\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        \n",
    "        is_cross_attention = encoder_hidden_states is not None\n",
    "\n",
    "        if is_cross_attention and past_key_value is not None:\n",
    "            raise Exception()\n",
    "        elif is_cross_attention:\n",
    "            raise Exception()\n",
    "        elif past_key_value is not None:\n",
    "            raise Exception()\n",
    "        else:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            raise Exception()\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        if self.print: print('SelfAttention.forward: last_attention_probs backuped')\n",
    "        self.last_attention_probs = attention_probs.detach().clone()\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, config, position_embedding_type=None):\n",
    "        super().__init__()\n",
    "        self.self = BertSelfAttention(config, position_embedding_type=position_embedding_type)\n",
    "        self.output = BertSelfOutput(config)\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        heads, index = find_pruneable_heads_and_indices(\n",
    "            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
    "        )\n",
    "\n",
    "        # Prune linear layers\n",
    "        self.self.query = prune_linear_layer(self.self.query, index)\n",
    "        self.self.key = prune_linear_layer(self.self.key, index)\n",
    "        self.self.value = prune_linear_layer(self.self.value, index)\n",
    "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
    "\n",
    "        # Update hyper params and store pruned heads\n",
    "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
    "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        self_outputs = self.self(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            encoder_hidden_states,\n",
    "            encoder_attention_mask,\n",
    "            past_key_value,\n",
    "            output_attentions,\n",
    "        )\n",
    "        attention_output = self.output(self_outputs[0], hidden_states)\n",
    "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.attention = BertAttention(config)\n",
    "        self.is_decoder = config.is_decoder\n",
    "        self.add_cross_attention = config.add_cross_attention\n",
    "        if self.add_cross_attention:\n",
    "            if not self.is_decoder:\n",
    "                raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n",
    "            self.crossattention = BertAttention(config, position_embedding_type=\"absolute\")\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
    "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
    "        self_attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            past_key_value=self_attn_past_key_value,\n",
    "        )\n",
    "        attention_output = self_attention_outputs[0]\n",
    "\n",
    "        # if decoder, the last output is tuple of self-attn cache\n",
    "        if self.is_decoder:\n",
    "            outputs = self_attention_outputs[1:-1]\n",
    "            present_key_value = self_attention_outputs[-1]\n",
    "        else:\n",
    "            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        cross_attn_present_key_value = None\n",
    "        if self.is_decoder and encoder_hidden_states is not None:\n",
    "            if not hasattr(self, \"crossattention\"):\n",
    "                raise ValueError(\n",
    "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"\n",
    "                )\n",
    "\n",
    "            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n",
    "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
    "            cross_attention_outputs = self.crossattention(\n",
    "                attention_output,\n",
    "                attention_mask,\n",
    "                head_mask,\n",
    "                encoder_hidden_states,\n",
    "                encoder_attention_mask,\n",
    "                cross_attn_past_key_value,\n",
    "                output_attentions,\n",
    "            )\n",
    "            attention_output = cross_attention_outputs[0]\n",
    "            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n",
    "\n",
    "            # add cross-attn cache to positions 3,4 of present_key_value tuple\n",
    "            cross_attn_present_key_value = cross_attention_outputs[-1]\n",
    "            present_key_value = present_key_value + cross_attn_present_key_value\n",
    "\n",
    "        layer_output = apply_chunking_to_forward(\n",
    "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
    "        )\n",
    "        outputs = (layer_output,) + outputs\n",
    "\n",
    "        # if decoder, return the attn key/values as the last output\n",
    "        if self.is_decoder:\n",
    "            outputs = outputs + (present_key_value,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def feed_forward_chunk(self, attention_output):\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output\n",
    "\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                if use_cache:\n",
    "                    logger.warning(\n",
    "                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                    )\n",
    "                    use_cache = False\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(layer_module),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    past_key_value,\n",
    "                    output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[-1],)\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "\n",
    "class BertPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "\n",
    "    config_class = BertConfig\n",
    "    #load_tf_weights = load_tf_weights_in_bert\n",
    "    base_model_prefix = \"bert\"\n",
    "    supports_gradient_checkpointing = True\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def _set_gradient_checkpointing(self, module, value=False):\n",
    "        if isinstance(module, BertEncoder):\n",
    "            module.gradient_checkpointing = value\n",
    "\n",
    "\n",
    "class BertModel(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "\n",
    "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
    "    cross-attention is added between the self-attention layers, following the architecture described in [Attention is\n",
    "    all you need](https://arxiv.org/abs/1706.03762) by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n",
    "    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n",
    "\n",
    "    To behave as an decoder the model needs to be initialized with the `is_decoder` argument of the configuration set\n",
    "    to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n",
    "    `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, add_pooling_layer=True):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "\n",
    "        self.pooler = BertPooler(config) if add_pooling_layer else None\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\"\n",
    "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
    "        class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "            \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
    "            the model is configured as a decoder.\n",
    "        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
    "            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
    "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
    "\n",
    "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
    "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
    "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
    "        use_cache (`bool`, *optional*):\n",
    "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
    "            `past_key_values`).\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if self.config.is_decoder:\n",
    "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        else:\n",
    "            use_cache = False\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        batch_size, seq_length = input_shape\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        # past_key_values_length\n",
    "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            if hasattr(self.embeddings, \"token_type_ids\"):\n",
    "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
    "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
    "                token_type_ids = buffered_token_type_ids_expanded\n",
    "            else:\n",
    "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
    "\n",
    "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            past_key_values_length=past_key_values_length,\n",
    "        )\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            past_key_values=encoder_outputs.past_key_values,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            cross_attentions=encoder_outputs.cross_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-4_H-256_A-4 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsebert = BertModel.from_pretrained(trainer.model.bert_model_name)\n",
    "sparsebert = sparsebert.to(trainer.device)\n",
    "sparsebert = sparsebert.eval()\n",
    "sparsebert.load_state_dict(bert.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7b3ac0126d0d6fea024471ce24e510948bf6332f7ae1a66cdcb4ee9887514e9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('tensorflow': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
