{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time, random, math, os, sys, tqdm\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from trainer.classification import Trainer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120000/120000 [00:00<00:00, 208311.01it/s]\n",
      "100%|██████████| 7600/7600 [00:00<00:00, 211105.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Dataset Stat.: name:AG_NEWS, nclass:5, max_len:1012, avg_len:236.477525, count:120000\n",
      "Classification Dataset Stat.: name:AG_NEWS, nclass:5, max_len:892, avg_len:235.2992105263158, count:7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-4_H-256_A-4 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer.__init__: Model initialized. model = bert\n",
      "Trainer.load: Loading...\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(batch_size=32)\n",
    "trainer.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = trainer.get_batch()\n",
    "trainer.model = trainer.model.eval()\n",
    "bert = trainer.model.bert\n",
    "fc = trainer.model.classifier\n",
    "\n",
    "def eval(model, batch=batch, fc=fc):\n",
    "    lm_output = model(\n",
    "        input_ids = batch.input_ids, \n",
    "        attention_mask = batch.attention_masks, \n",
    "        output_hidden_states=True,\n",
    "        output_attentions=True,\n",
    "    )\n",
    "    last_hidden = lm_output.last_hidden_state[:,0,:]\n",
    "    x = fc(last_hidden)\n",
    "    return torch.argmax(x, dim=-1), batch.labels, lm_output\n",
    "output, labels, lm_output = eval(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1.0000001192092896, 34, torch.Size([32, 4, 290, 290]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LAYER = -1\n",
    "BATCH = 0\n",
    "HEAD = 0\n",
    "N = torch.sum(batch.attention_masks[BATCH]).item()\n",
    "\n",
    "plt.imshow(lm_output.attentions[LAYER][BATCH][HEAD][:N, :N].detach().cpu().numpy())\n",
    "plt.show()\n",
    "\n",
    "torch.sum(lm_output.attentions[LAYER][BATCH][HEAD][0,:N]).item(), N, lm_output.attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "impact factor torch.Size([34])\n",
      "hign impact [15 18  3 13 17]\n",
      "0.07200193405151367\n",
      "tensor([15, 18,  3, 13, 51, 17, 44, 41, 19, 20], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, 14, 19, 18, 43, 17, 50, 16, 40, 24, 59, 21,  3, 27, 26, 44, 52, 30,\n",
      "        23, 12], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "NBATCH = batch.input_ids.shape[0]\n",
    "LAYER = 3\n",
    "attend_tokens = [0]\n",
    "\n",
    "a = lm_output.attentions[LAYER][BATCH][:,:N, :N]\n",
    "a = torch.sum(a, dim=0)\n",
    "a = a[attend_tokens,:]\n",
    "a = torch.sum(a, dim=0)\n",
    "print('impact factor', a.shape)\n",
    "b = torch.topk(a, 5)[1]\n",
    "print('hign impact', b.cpu().numpy())\n",
    "# plt.bar(range(N), a.detach().cpu().numpy())\n",
    "# plt.show()\n",
    "\n",
    "#this is very evil code...\n",
    "def update_input_mask_from_previous_attention(attention_mask, previous_attention, output_token_indices, output_token_impact, k=5):\n",
    "    NBATCH = attention_mask.shape[0]\n",
    "\n",
    "    input_indcies = []\n",
    "    input_impacts = []\n",
    "    for i, idxs in enumerate(output_token_indices):\n",
    "        N = int(torch.sum((attention_mask[i]==0)*1.0).item())\n",
    "        \n",
    "        #print(i, previous_attention, N)\n",
    "        a = previous_attention[i,:,:N,:N]\n",
    "        #print(a.shape, N, previous_attention.shape)\n",
    "        a = torch.sum(a, dim=0)\n",
    "        #print(idxs, a.shape)\n",
    "        a = a[idxs, :] * output_token_impact[i].view(-1, 1)\n",
    "        a = torch.sum(a, dim=0)\n",
    "        \n",
    "        #print(f'k {k}, a {a.shape}')\n",
    "        kxx = k\n",
    "        if k < 1.0: kxx = int(math.ceil(k*N))\n",
    "        b, c = torch.topk(a, min(N, kxx))\n",
    "        input_indcies.append(c)\n",
    "        input_impacts.append(b)\n",
    "    \n",
    "    input_mask = torch.zeros(NBATCH, attention_mask.shape[-1], device=previous_attention.device, dtype=previous_attention.dtype)\n",
    "    for i, idxs in enumerate(input_indcies):\n",
    "        for k in idxs:\n",
    "            input_mask[i, k] = 1.0\n",
    "    \n",
    "    return input_mask, input_indcies, input_impacts\n",
    "\n",
    "t = time.time()\n",
    "mask, indices, impacts = update_input_mask_from_previous_attention(\n",
    "    batch.attention_masks - 1.0, \n",
    "    lm_output.attentions[LAYER], \n",
    "    [[0]] * NBATCH, \n",
    "    torch.ones(NBATCH, 1, device=batch.device, dtype=torch.float32), \n",
    "    k=10\n",
    ")\n",
    "mask2, indices2, impacts2 = update_input_mask_from_previous_attention(\n",
    "    batch.attention_masks - 1.0, \n",
    "    lm_output.attentions[LAYER-1], \n",
    "    indices, \n",
    "    impacts, \n",
    "    k=20\n",
    ")\n",
    "print(time.time() - t)\n",
    "mask2.shape, batch.attention_masks.shape\n",
    "def plot_grid(grid):\n",
    "    plt.imshow(grid.cpu().detach().numpy())\n",
    "    #plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "print(indices[0])\n",
    "last_output_attentions = torch.sum(lm_output.attentions[LAYER][0], dim=0)[[0], :N]\n",
    "plot_grid(last_output_attentions)\n",
    "last_output_attentions_input_masked = last_output_attentions * mask[0][:N]\n",
    "plot_grid(last_output_attentions_input_masked)\n",
    "\n",
    "print(indices2[0])\n",
    "second_output_attentions = torch.sum(lm_output.attentions[LAYER-1][0], dim=0)[indices[0],:N]*impacts[0].view(-1,1)\n",
    "plot_grid(second_output_attentions)\n",
    "second_output_attentions_impact = torch.sum(second_output_attentions, dim=0).view(1,-1)\n",
    "plot_grid(second_output_attentions_impact)\n",
    "plot_grid(mask2[0][:N].view(1,-1))\n",
    "plot_grid(second_output_attentions_impact * mask2[0][:N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, os, torch\n",
    "import torch.utils.checkpoint\n",
    "from packaging import version\n",
    "from torch import nn\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    BaseModelOutputWithPoolingAndCrossAttentions,\n",
    ")\n",
    "from transformers.modeling_utils import (\n",
    "    PreTrainedModel,\n",
    "    apply_chunking_to_forward,\n",
    "    find_pruneable_heads_and_indices,\n",
    "    prune_linear_layer,\n",
    ")\n",
    "from transformers.utils import logging\n",
    "from transformers.models.bert.configuration_bert import BertConfig\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "from transformers.models.bert.modeling_bert import BertEmbeddings\n",
    "\n",
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config, position_embedding_type=None):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
    "                f\"heads ({config.num_attention_heads})\"\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.position_embedding_type = position_embedding_type or getattr(\n",
    "            config, \"position_embedding_type\", \"absolute\"\n",
    "        )\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            raise Exception('removed')\n",
    "\n",
    "        self.is_decoder = config.is_decoder\n",
    "        if self.is_decoder: raise Exception()\n",
    "\n",
    "        self.print = True\n",
    "        self.input_mask = None\n",
    "        self.input_indices = None\n",
    "        self.input_impacts = None\n",
    "        self.attention_masking_timing = 'after_softmax'\n",
    "    \n",
    "    def reset_input_mask(self):\n",
    "        self.input_mask = None\n",
    "        self.input_indices = None\n",
    "        self.input_impacts = None\n",
    "    \n",
    "    def update_input_mask_from_previous_attention(self, output_token_indices, output_token_impact, k):\n",
    "        input_mask, input_indcies, input_impacts = update_input_mask_from_previous_attention(\n",
    "            self.last_attention_mask, \n",
    "            self.last_attention_probs, \n",
    "            output_token_indices, \n",
    "            output_token_impact, \n",
    "            k\n",
    "        )\n",
    "        self.input_mask = input_mask\n",
    "        self.input_indices = input_indcies\n",
    "        self.input_impacts = input_impacts\n",
    "        return input_mask, input_indcies, input_impacts\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        if self.is_decoder: raise Exception()\n",
    "        if not self.attention_masking_timing in ['after_softmax', 'before_softmax']: raise Exception()\n",
    "\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        \n",
    "        is_cross_attention = encoder_hidden_states is not None\n",
    "\n",
    "        if (is_cross_attention and past_key_value is not None) or is_cross_attention or (past_key_value is not None):\n",
    "            raise Exception()\n",
    "        else:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            raise Exception()\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        if self.input_mask is not None and self.attention_masking_timing == 'before_softmax':\n",
    "            if self.print: print(f'apply input mask, before softmax. input_mask:{self.input_mask.shape}, attention_scores:{attention_scores.shape}')\n",
    "            attention_scores = attention_scores * self.input_mask.view(self.input_mask.shape[0], 1, 1, self.input_mask.shape[-1])\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "        \n",
    "        if self.print: print('SelfAttention.forward: last_attention_probs backuped')\n",
    "        self.last_attention_probs = attention_probs.detach().clone()\n",
    "        self.last_attention_mask = attention_mask.detach().clone()\n",
    "\n",
    "        if self.input_mask is not None and self.attention_masking_timing == 'after_softmax':\n",
    "            if self.print: print(f'apply input mask, after softmax. input_mask:{self.input_mask.shape}, attention_probs:{attention_probs.shape}')\n",
    "            attention_probs = attention_probs * self.input_mask.view(self.input_mask.shape[0], 1, 1, self.input_mask.shape[-1])\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, config, position_embedding_type=None):\n",
    "        super().__init__()\n",
    "        self.self = BertSelfAttention(config, position_embedding_type=position_embedding_type)\n",
    "        self.output = BertSelfOutput(config)\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        heads, index = find_pruneable_heads_and_indices(\n",
    "            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
    "        )\n",
    "\n",
    "        # Prune linear layers\n",
    "        self.self.query = prune_linear_layer(self.self.query, index)\n",
    "        self.self.key = prune_linear_layer(self.self.key, index)\n",
    "        self.self.value = prune_linear_layer(self.self.value, index)\n",
    "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
    "\n",
    "        # Update hyper params and store pruned heads\n",
    "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
    "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        self_outputs = self.self(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            encoder_hidden_states,\n",
    "            encoder_attention_mask,\n",
    "            past_key_value,\n",
    "            output_attentions,\n",
    "        )\n",
    "        attention_output = self.output(self_outputs[0], hidden_states)\n",
    "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.attention = BertAttention(config)\n",
    "        self.is_decoder = config.is_decoder\n",
    "        self.add_cross_attention = config.add_cross_attention\n",
    "        if self.add_cross_attention:\n",
    "            if not self.is_decoder:\n",
    "                raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n",
    "            self.crossattention = BertAttention(config, position_embedding_type=\"absolute\")\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
    "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
    "        self_attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            past_key_value=self_attn_past_key_value,\n",
    "        )\n",
    "        attention_output = self_attention_outputs[0]\n",
    "\n",
    "        # if decoder, the last output is tuple of self-attn cache\n",
    "        if self.is_decoder:\n",
    "            outputs = self_attention_outputs[1:-1]\n",
    "            present_key_value = self_attention_outputs[-1]\n",
    "        else:\n",
    "            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        cross_attn_present_key_value = None\n",
    "        if self.is_decoder and encoder_hidden_states is not None:\n",
    "            if not hasattr(self, \"crossattention\"):\n",
    "                raise ValueError(\n",
    "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"\n",
    "                )\n",
    "\n",
    "            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n",
    "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
    "            cross_attention_outputs = self.crossattention(\n",
    "                attention_output,\n",
    "                attention_mask,\n",
    "                head_mask,\n",
    "                encoder_hidden_states,\n",
    "                encoder_attention_mask,\n",
    "                cross_attn_past_key_value,\n",
    "                output_attentions,\n",
    "            )\n",
    "            attention_output = cross_attention_outputs[0]\n",
    "            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n",
    "\n",
    "            # add cross-attn cache to positions 3,4 of present_key_value tuple\n",
    "            cross_attn_present_key_value = cross_attention_outputs[-1]\n",
    "            present_key_value = present_key_value + cross_attn_present_key_value\n",
    "\n",
    "        layer_output = apply_chunking_to_forward(\n",
    "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
    "        )\n",
    "        outputs = (layer_output,) + outputs\n",
    "\n",
    "        # if decoder, return the attn key/values as the last output\n",
    "        if self.is_decoder:\n",
    "            outputs = outputs + (present_key_value,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def feed_forward_chunk(self, attention_output):\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output\n",
    "\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                if use_cache:\n",
    "                    logger.warning(\n",
    "                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                    )\n",
    "                    use_cache = False\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(layer_module),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    past_key_value,\n",
    "                    output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[-1],)\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "\n",
    "class BertPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "\n",
    "    config_class = BertConfig\n",
    "    #load_tf_weights = load_tf_weights_in_bert\n",
    "    base_model_prefix = \"bert\"\n",
    "    supports_gradient_checkpointing = True\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def _set_gradient_checkpointing(self, module, value=False):\n",
    "        if isinstance(module, BertEncoder):\n",
    "            module.gradient_checkpointing = value\n",
    "\n",
    "\n",
    "class BertModel(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "\n",
    "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
    "    cross-attention is added between the self-attention layers, following the architecture described in [Attention is\n",
    "    all you need](https://arxiv.org/abs/1706.03762) by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n",
    "    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n",
    "\n",
    "    To behave as an decoder the model needs to be initialized with the `is_decoder` argument of the configuration set\n",
    "    to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n",
    "    `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, add_pooling_layer=True):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "\n",
    "        self.pooler = BertPooler(config) if add_pooling_layer else None\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\"\n",
    "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
    "        class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "            \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
    "            the model is configured as a decoder.\n",
    "        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
    "            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
    "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
    "\n",
    "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
    "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
    "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
    "        use_cache (`bool`, *optional*):\n",
    "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
    "            `past_key_values`).\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if self.config.is_decoder:\n",
    "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        else:\n",
    "            use_cache = False\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        batch_size, seq_length = input_shape\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        # past_key_values_length\n",
    "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            if hasattr(self.embeddings, \"token_type_ids\"):\n",
    "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
    "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
    "                token_type_ids = buffered_token_type_ids_expanded\n",
    "            else:\n",
    "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
    "\n",
    "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            past_key_values_length=past_key_values_length,\n",
    "        )\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            past_key_values=encoder_outputs.past_key_values,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            cross_attentions=encoder_outputs.cross_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttention.forward: last_attention_probs backuped\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "SelfAttention.forward: last_attention_probs backuped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1, 4, 4, 2, 2, 1, 1, 2, 4, 1, 4, 3, 4, 2, 2, 3, 4, 4, 2, 1, 1, 2, 2, 3,\n",
       "         4, 2, 3, 4, 3, 4, 3, 1], device='cuda:0'),\n",
       " tensor([1, 4, 4, 2, 2, 1, 1, 2, 4, 1, 4, 3, 4, 2, 2, 3, 4, 4, 2, 1, 1, 2, 2, 3,\n",
       "         4, 2, 3, 4, 3, 4, 3, 1], device='cuda:0'))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsebert = BertModel(trainer.model.bert.config)\n",
    "sparsebert = sparsebert.to(trainer.device)\n",
    "sparsebert = sparsebert.eval()\n",
    "sparsebert.load_state_dict(bert.state_dict())\n",
    "eval(sparsebert)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttention.forward: last_attention_probs backuped\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "apply input mask, before softmax. input_mask:torch.Size([32, 120]), attention_scores:torch.Size([32, 4, 120, 120])\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "apply input mask, before softmax. input_mask:torch.Size([32, 120]), attention_scores:torch.Size([32, 4, 120, 120])\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "apply input mask, before softmax. input_mask:torch.Size([32, 120]), attention_scores:torch.Size([32, 4, 120, 120])\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "apply input mask, before softmax. input_mask:torch.Size([32, 120]), attention_scores:torch.Size([32, 4, 120, 120])\n",
      "SelfAttention.forward: last_attention_probs backuped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1, 3, 3, 2, 2, 1, 1, 2, 4, 1, 4, 3, 4, 2, 2, 3, 4, 4, 2, 1, 1, 2, 2, 3,\n",
       "         4, 2, 3, 4, 3, 4, 3, 1], device='cuda:0'),\n",
       " tensor([1, 4, 4, 2, 2, 1, 1, 2, 4, 1, 4, 3, 4, 2, 2, 3, 4, 4, 2, 1, 1, 2, 2, 3,\n",
       "         4, 2, 3, 4, 3, 4, 3, 1], device='cuda:0'))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def set_print(sparsebert, v):\n",
    "    for layer in sparsebert.encoder.layer:\n",
    "        layer.attention.self.print = v\n",
    "\n",
    "def set_masking_timing(sparsebert, v):\n",
    "    for layer in sparsebert.encoder.layer:\n",
    "        layer.attention.self.attention_masking_timing = v\n",
    "\n",
    "def update_input_mask(sparsebert, ks=[0.999,0.5,0.25,0.1]):\n",
    "    dtype = sparsebert.encoder.layer[0].attention.self.last_attention_probs.dtype\n",
    "    device = sparsebert.encoder.layer[0].attention.self.last_attention_probs.device\n",
    "    batch_size = sparsebert.encoder.layer[0].attention.self.last_attention_mask.shape[0]\n",
    "    indices = torch.zeros(batch_size, 1, dtype=torch.int64, device=device)\n",
    "    impacts = torch.ones(batch_size, 1, dtype=dtype, device=device)\n",
    "    L = len(sparsebert.encoder.layer)\n",
    "    for i in range(L):\n",
    "        mask, indices, impacts = sparsebert.encoder.layer[L-i-1].attention.self.update_input_mask_from_previous_attention(\n",
    "            output_token_indices = indices,\n",
    "            output_token_impact = impacts,\n",
    "            k = ks[L-i-1],\n",
    "        )\n",
    "\n",
    "def reset_input_mask(sparsebert):\n",
    "    for layer in sparsebert.encoder.layer:\n",
    "        layer.attention.self.reset_input_mask()\n",
    "\n",
    "set_masking_timing(sparsebert, 'before_softmax')\n",
    "reset_input_mask(sparsebert)\n",
    "eval(sparsebert)\n",
    "update_input_mask(sparsebert)\n",
    "ret = eval(sparsebert)[:2]\n",
    "reset_input_mask(sparsebert)\n",
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(60., device='cuda:0')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(((sparsebert.encoder.layer[0].attention.self.last_attention_mask[0] == 0)*1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 81.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "93.21875"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(bert, eval=eval):\n",
    "    trainer.seed()\n",
    "    acc_sum = 0\n",
    "    N = 100\n",
    "    for i in tqdm.tqdm(range(N)):\n",
    "        batch = trainer.get_batch(test=True)\n",
    "        output, label, _ = eval(bert, batch=batch)\n",
    "        acc_sum += torch.mean((output == label) * 1.0)\n",
    "    return acc_sum.item()\n",
    "accuracy(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:25<00:00,  3.93it/s]\n",
      "100%|██████████| 100/100 [00:25<00:00,  3.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(92.28125, 90.78125)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sparse_eval(sparsebert, batch=batch):\n",
    "    reset_input_mask(sparsebert)\n",
    "    eval(sparsebert, batch=batch)\n",
    "    update_input_mask(sparsebert)\n",
    "    ret = eval(sparsebert, batch=batch)\n",
    "    reset_input_mask(sparsebert)\n",
    "    return ret\n",
    "set_masking_timing(sparsebert, 'after_softmax')\n",
    "set_print(sparsebert, False)\n",
    "acc_after = accuracy(sparsebert, eval=sparse_eval)\n",
    "set_print(sparsebert, True)\n",
    "set_masking_timing(sparsebert, 'before_softmax')\n",
    "set_print(sparsebert, False)\n",
    "acc_before = accuracy(sparsebert, eval=sparse_eval)\n",
    "set_print(sparsebert, True)\n",
    "acc_after, acc_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttention.forward: last_attention_probs backuped\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "apply input mask, after softmax. input_mask:torch.Size([32, 120]), attention_probs:torch.Size([32, 4, 120, 120])\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "apply input mask, after softmax. input_mask:torch.Size([32, 120]), attention_probs:torch.Size([32, 4, 120, 120])\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "apply input mask, after softmax. input_mask:torch.Size([32, 120]), attention_probs:torch.Size([32, 4, 120, 120])\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "apply input mask, after softmax. input_mask:torch.Size([32, 120]), attention_probs:torch.Size([32, 4, 120, 120])\n",
      "Attention MSE tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "set_masking_timing(sparsebert, 'after_softmax')\n",
    "reset_input_mask(sparsebert)\n",
    "_,_, lm_output = eval(sparsebert, batch=batch)\n",
    "update_input_mask(sparsebert)\n",
    "_,_, lm_output_sparse = eval(sparsebert, batch=batch)\n",
    "reset_input_mask(sparsebert)\n",
    "print('Attention MSE', torch.mean(torch.square(lm_output.attentions[-1]-lm_output_sparse.attentions[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7b3ac0126d0d6fea024471ce24e510948bf6332f7ae1a66cdcb4ee9887514e9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('tensorflow': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
