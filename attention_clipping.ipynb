{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time, random, math, os, sys, tqdm\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from trainer.classification import Trainer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120000/120000 [00:00<00:00, 214283.64it/s]\n",
      "100%|██████████| 7600/7600 [00:00<00:00, 205404.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Dataset Stat.: name:AG_NEWS, nclass:5, max_len:1012, avg_len:236.477525, count:120000\n",
      "Classification Dataset Stat.: name:AG_NEWS, nclass:5, max_len:892, avg_len:235.2992105263158, count:7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-4_H-256_A-4 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer.__init__: Model initialized. model = bert\n",
      "Trainer.load: Loading...\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(batch_size=8)\n",
    "trainer.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = trainer.get_batch()\n",
    "trainer.model = trainer.model.eval()\n",
    "bert = trainer.model.bert\n",
    "fc = trainer.model.classifier\n",
    "\n",
    "def eval(model, batch=batch, fc=fc):\n",
    "    lm_output = model(\n",
    "        input_ids = batch.input_ids, \n",
    "        attention_mask = batch.attention_masks, \n",
    "        output_hidden_states=True,\n",
    "        output_attentions=True,\n",
    "    )\n",
    "    last_hidden = lm_output.last_hidden_state[:,0,:]\n",
    "    x = fc(last_hidden)\n",
    "    return torch.argmax(x, dim=-1), batch.labels, lm_output\n",
    "output, labels, lm_output = eval(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 34, torch.Size([8, 4, 71, 71]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LAYER = -1\n",
    "BATCH = 0\n",
    "HEAD = 0\n",
    "N = torch.sum(batch.attention_masks[BATCH]).item()\n",
    "\n",
    "plt.imshow(lm_output.attentions[LAYER][BATCH][HEAD][:N, :N].detach().cpu().numpy())\n",
    "plt.show()\n",
    "\n",
    "torch.sum(lm_output.attentions[LAYER][BATCH][HEAD][0,:N]).item(), N, lm_output.attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "impact factor torch.Size([34])\n",
      "hign impact [33  0 27 18  7]\n",
      "0.01699972152709961\n",
      "tensor([33,  0, 27, 18,  7, 17, 12, 29, 24, 32], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, 27, 33, 14, 31, 21, 16,  6, 18, 29,  8, 25,  5,  7, 32, 13, 28, 17,\n",
      "         1,  4], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "NBATCH = batch.input_ids.shape[0]\n",
    "LAYER = 3\n",
    "attend_tokens = [0]\n",
    "\n",
    "a = lm_output.attentions[LAYER][BATCH][:,:N, :N]\n",
    "a = torch.sum(a, dim=0)\n",
    "a = a[attend_tokens,:]\n",
    "a = torch.sum(a, dim=0)\n",
    "print('impact factor', a.shape)\n",
    "b = torch.topk(a, 5)[1]\n",
    "print('hign impact', b.cpu().numpy())\n",
    "# plt.bar(range(N), a.detach().cpu().numpy())\n",
    "# plt.show()\n",
    "\n",
    "#this is very evil code...\n",
    "def update_input_mask_from_previous_attention(attention_mask, previous_attention, output_token_indices, output_token_impact, k=5):\n",
    "    NBATCH = attention_mask.shape[0]\n",
    "\n",
    "    input_indcies = []\n",
    "    input_impacts = []\n",
    "    for i, idxs in enumerate(output_token_indices):\n",
    "        N = int(torch.sum((attention_mask[i]==0)*1.0).item())\n",
    "        \n",
    "        #print(i, previous_attention, N)\n",
    "        a = previous_attention[i,:,:N,:N]\n",
    "        #print(a.shape, N, previous_attention.shape)\n",
    "        a = torch.sum(a, dim=0)\n",
    "        #print(idxs, a.shape)\n",
    "        a = a[idxs, :] * output_token_impact[i].view(-1, 1)\n",
    "        a = torch.sum(a, dim=0)\n",
    "        \n",
    "        #print(f'k {k}, a {a.shape}')\n",
    "        kxx = k\n",
    "        if k < 1.0: kxx = int(math.ceil(k*N))\n",
    "        b, c = torch.topk(a, min(N, kxx))\n",
    "        input_indcies.append(c)\n",
    "        input_impacts.append(b)\n",
    "    \n",
    "    input_mask = torch.zeros(NBATCH, attention_mask.shape[-1], device=previous_attention.device, dtype=previous_attention.dtype)\n",
    "    for i, idxs in enumerate(input_indcies):\n",
    "        for k in idxs:\n",
    "            input_mask[i, k] = 1.0\n",
    "    \n",
    "    return input_mask, input_indcies, input_impacts\n",
    "\n",
    "t = time.time()\n",
    "mask, indices, impacts = update_input_mask_from_previous_attention(\n",
    "    batch.attention_masks - 1.0, \n",
    "    lm_output.attentions[LAYER], \n",
    "    [[0]] * NBATCH, \n",
    "    torch.ones(NBATCH, 1, device=batch.device, dtype=torch.float32), \n",
    "    k=10\n",
    ")\n",
    "mask2, indices2, impacts2 = update_input_mask_from_previous_attention(\n",
    "    batch.attention_masks - 1.0, \n",
    "    lm_output.attentions[LAYER-1], \n",
    "    indices, \n",
    "    impacts, \n",
    "    k=20\n",
    ")\n",
    "print(time.time() - t)\n",
    "mask2.shape, batch.attention_masks.shape\n",
    "def plot_grid(grid):\n",
    "    plt.imshow(grid.cpu().detach().numpy())\n",
    "    #plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "print(indices[0])\n",
    "last_output_attentions = torch.sum(lm_output.attentions[LAYER][0], dim=0)[[0], :N]\n",
    "plot_grid(last_output_attentions)\n",
    "last_output_attentions_input_masked = last_output_attentions * mask[0][:N]\n",
    "plot_grid(last_output_attentions_input_masked)\n",
    "\n",
    "print(indices2[0])\n",
    "second_output_attentions = torch.sum(lm_output.attentions[LAYER-1][0], dim=0)[indices[0],:N]*impacts[0].view(-1,1)\n",
    "plot_grid(second_output_attentions)\n",
    "second_output_attentions_impact = torch.sum(second_output_attentions, dim=0).view(1,-1)\n",
    "plot_grid(second_output_attentions_impact)\n",
    "plot_grid(mask2[0][:N].view(1,-1))\n",
    "plot_grid(second_output_attentions_impact * mask2[0][:N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, os, torch\n",
    "import torch.utils.checkpoint\n",
    "from packaging import version\n",
    "from torch import nn\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    BaseModelOutputWithPoolingAndCrossAttentions,\n",
    ")\n",
    "from transformers.modeling_utils import (\n",
    "    PreTrainedModel,\n",
    "    apply_chunking_to_forward,\n",
    "    find_pruneable_heads_and_indices,\n",
    "    prune_linear_layer,\n",
    ")\n",
    "from transformers.utils import logging\n",
    "from transformers.models.bert.configuration_bert import BertConfig\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "from transformers.models.bert.modeling_bert import BertEmbeddings\n",
    "\n",
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config, position_embedding_type=None):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
    "                f\"heads ({config.num_attention_heads})\"\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.position_embedding_type = position_embedding_type or getattr(\n",
    "            config, \"position_embedding_type\", \"absolute\"\n",
    "        )\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            raise Exception('removed')\n",
    "\n",
    "        self.is_decoder = config.is_decoder\n",
    "        if self.is_decoder: raise Exception()\n",
    "\n",
    "        self.print = True\n",
    "        self.reset_input_mask()\n",
    "        self.attention_masking_timing = 'after_softmax'\n",
    "        self.output_masking = True\n",
    "    \n",
    "    def reset_input_mask(self):\n",
    "        self.input_mask = None\n",
    "        self.input_indices = None\n",
    "        self.input_impacts = None\n",
    "        self.output_mask = None\n",
    "    \n",
    "    def update_input_mask_from_previous_attention(self, output_token_mask, output_token_indices, output_token_impact, k):\n",
    "        input_mask, input_indcies, input_impacts = update_input_mask_from_previous_attention(\n",
    "            self.last_attention_mask, \n",
    "            self.last_attention_probs, \n",
    "            output_token_indices, \n",
    "            output_token_impact, \n",
    "            k\n",
    "        )\n",
    "        self.input_mask = input_mask\n",
    "        self.input_indices = input_indcies\n",
    "        self.input_impacts = input_impacts\n",
    "        self.output_mask = output_token_mask\n",
    "        return input_mask, input_indcies, input_impacts\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        if self.is_decoder: raise Exception()\n",
    "        if not self.attention_masking_timing in ['after_softmax', 'before_softmax']: raise Exception()\n",
    "        self.last_hidden_states = hidden_states\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        \n",
    "        is_cross_attention = encoder_hidden_states is not None\n",
    "\n",
    "        if (is_cross_attention and past_key_value is not None) or is_cross_attention or (past_key_value is not None):\n",
    "            raise Exception()\n",
    "        else:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            raise Exception()\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        if self.input_mask is not None and self.attention_masking_timing == 'before_softmax':\n",
    "            if self.print: print(f'apply input mask, before softmax. input_mask:{self.input_mask.shape}, attention_scores:{attention_scores.shape}')\n",
    "            attention_scores = attention_scores + (1.0 - self.input_mask.view(self.input_mask.shape[0], 1, 1, self.input_mask.shape[-1])) * -10000\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "        \n",
    "        if self.print: print('SelfAttention.forward: last_attention_probs backuped')\n",
    "        self.last_attention_probs = attention_probs.detach().clone()\n",
    "        self.last_attention_mask = attention_mask.detach().clone()\n",
    "\n",
    "        if self.input_mask is not None and self.attention_masking_timing == 'after_softmax':\n",
    "            if self.print: print(f'apply input mask, after softmax. input_mask:{self.input_mask.shape}, attention_probs:{attention_probs.shape}')\n",
    "            attention_probs = attention_probs * self.input_mask.view(self.input_mask.shape[0], 1, 1, self.input_mask.shape[-1])\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        if self.output_mask is not None and self.output_masking:\n",
    "            output_mask = self.output_mask.unsqueeze(-1)\n",
    "            if self.print: print(f'apply output mask. mask:{output_mask.shape} context:{context_layer.shape}')\n",
    "            context_layer = context_layer * output_mask\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, config, position_embedding_type=None):\n",
    "        super().__init__()\n",
    "        self.self = BertSelfAttention(config, position_embedding_type=position_embedding_type)\n",
    "        self.output = BertSelfOutput(config)\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        heads, index = find_pruneable_heads_and_indices(\n",
    "            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
    "        )\n",
    "\n",
    "        # Prune linear layers\n",
    "        self.self.query = prune_linear_layer(self.self.query, index)\n",
    "        self.self.key = prune_linear_layer(self.self.key, index)\n",
    "        self.self.value = prune_linear_layer(self.self.value, index)\n",
    "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
    "\n",
    "        # Update hyper params and store pruned heads\n",
    "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
    "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        self_outputs = self.self(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            encoder_hidden_states,\n",
    "            encoder_attention_mask,\n",
    "            past_key_value,\n",
    "            output_attentions,\n",
    "        )\n",
    "        attention_output = self.output(self_outputs[0], hidden_states)\n",
    "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.attention = BertAttention(config)\n",
    "        self.is_decoder = config.is_decoder\n",
    "        self.add_cross_attention = config.add_cross_attention\n",
    "        if self.add_cross_attention:\n",
    "            if not self.is_decoder:\n",
    "                raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n",
    "            self.crossattention = BertAttention(config, position_embedding_type=\"absolute\")\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
    "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
    "        self_attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            past_key_value=self_attn_past_key_value,\n",
    "        )\n",
    "        self.self_attention_outputs = self_attention_outputs\n",
    "        attention_output = self_attention_outputs[0]\n",
    "\n",
    "        # if decoder, the last output is tuple of self-attn cache\n",
    "        if self.is_decoder: raise Exception()\n",
    "        else: outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        cross_attn_present_key_value = None\n",
    "        if self.is_decoder and encoder_hidden_states is not None: raise Exception()\n",
    "\n",
    "        layer_output = apply_chunking_to_forward(\n",
    "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
    "        )\n",
    "        self.layer_output = layer_output\n",
    "        outputs = (layer_output,) + outputs\n",
    "\n",
    "        # if decoder, return the attn key/values as the last output\n",
    "        if self.is_decoder: raise Exception()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def feed_forward_chunk(self, attention_output):\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output\n",
    "\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                if use_cache:\n",
    "                    logger.warning(\n",
    "                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                    )\n",
    "                    use_cache = False\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(layer_module),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    past_key_value,\n",
    "                    output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[-1],)\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "\n",
    "class BertPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "\n",
    "    config_class = BertConfig\n",
    "    #load_tf_weights = load_tf_weights_in_bert\n",
    "    base_model_prefix = \"bert\"\n",
    "    supports_gradient_checkpointing = True\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize the weights\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def _set_gradient_checkpointing(self, module, value=False):\n",
    "        if isinstance(module, BertEncoder):\n",
    "            module.gradient_checkpointing = value\n",
    "\n",
    "\n",
    "class BertModel(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "\n",
    "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
    "    cross-attention is added between the self-attention layers, following the architecture described in [Attention is\n",
    "    all you need](https://arxiv.org/abs/1706.03762) by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n",
    "    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n",
    "\n",
    "    To behave as an decoder the model needs to be initialized with the `is_decoder` argument of the configuration set\n",
    "    to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n",
    "    `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, add_pooling_layer=True):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "\n",
    "        self.pooler = BertPooler(config) if add_pooling_layer else None\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\"\n",
    "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
    "        class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "            \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
    "            the model is configured as a decoder.\n",
    "        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
    "            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
    "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
    "\n",
    "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
    "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
    "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
    "        use_cache (`bool`, *optional*):\n",
    "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
    "            `past_key_values`).\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if self.config.is_decoder:\n",
    "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        else:\n",
    "            use_cache = False\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        batch_size, seq_length = input_shape\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        # past_key_values_length\n",
    "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            if hasattr(self.embeddings, \"token_type_ids\"):\n",
    "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
    "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
    "                token_type_ids = buffered_token_type_ids_expanded\n",
    "            else:\n",
    "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
    "\n",
    "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            past_key_values_length=past_key_values_length,\n",
    "        )\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            past_key_values=encoder_outputs.past_key_values,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            cross_attentions=encoder_outputs.cross_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttention.forward: last_attention_probs backuped\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "SelfAttention.forward: last_attention_probs backuped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 4, 1, 3, 4, 3, 3], device='cuda:0'),\n",
       " tensor([1, 2, 4, 1, 3, 4, 3, 3], device='cuda:0'))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsebert = BertModel(trainer.model.bert.config)\n",
    "sparsebert = sparsebert.to(trainer.device)\n",
    "sparsebert = sparsebert.eval()\n",
    "sparsebert.load_state_dict(bert.state_dict())\n",
    "eval(sparsebert)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttention.forward: last_attention_probs backuped\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "apply input mask, before softmax. input_mask:torch.Size([8, 71]), attention_scores:torch.Size([8, 4, 71, 71])\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "apply input mask, before softmax. input_mask:torch.Size([8, 71]), attention_scores:torch.Size([8, 4, 71, 71])\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "apply input mask, before softmax. input_mask:torch.Size([8, 71]), attention_scores:torch.Size([8, 4, 71, 71])\n",
      "SelfAttention.forward: last_attention_probs backuped\n",
      "apply input mask, before softmax. input_mask:torch.Size([8, 71]), attention_scores:torch.Size([8, 4, 71, 71])\n",
      "SelfAttention.forward: last_attention_probs backuped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 4, 1, 3, 4, 3, 3], device='cuda:0'),\n",
       " tensor([1, 2, 4, 1, 3, 4, 3, 3], device='cuda:0'))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def set_print(sparsebert, v):\n",
    "    for layer in sparsebert.encoder.layer:\n",
    "        layer.attention.self.print = v\n",
    "\n",
    "def set_masking_timing(sparsebert, v):\n",
    "    for layer in sparsebert.encoder.layer:\n",
    "        layer.attention.self.attention_masking_timing = v\n",
    "\n",
    "def set_output_masking(sparsebert, v):\n",
    "    for layer in sparsebert.encoder.layer:\n",
    "        layer.attention.self.output_masking = v\n",
    "\n",
    "def update_input_mask(sparsebert, ks=[0.999,0.5,0.25,0.1]):\n",
    "    dtype = sparsebert.encoder.layer[0].attention.self.last_attention_probs.dtype\n",
    "    device = sparsebert.encoder.layer[0].attention.self.last_attention_probs.device\n",
    "    batch_size = sparsebert.encoder.layer[0].attention.self.last_attention_mask.shape[0]\n",
    "    token_len = sparsebert.encoder.layer[0].attention.self.last_attention_mask.shape[-1]\n",
    "    mask = torch.zeros(batch_size, token_len, dtype=dtype, device=device)\n",
    "    mask[:,0] = 1.0\n",
    "    indices = torch.zeros(batch_size, 1, dtype=torch.int64, device=device)\n",
    "    impacts = torch.ones(batch_size, 1, dtype=dtype, device=device)\n",
    "    L = len(sparsebert.encoder.layer)\n",
    "    for i in range(L):\n",
    "        mask, indices, impacts = sparsebert.encoder.layer[L-i-1].attention.self.update_input_mask_from_previous_attention(\n",
    "            output_token_mask = mask,\n",
    "            output_token_indices = indices,\n",
    "            output_token_impact = impacts,\n",
    "            k = ks[L-i-1],\n",
    "        )\n",
    "\n",
    "def reset_input_mask(sparsebert):\n",
    "    for layer in sparsebert.encoder.layer:\n",
    "        layer.attention.self.reset_input_mask()\n",
    "\n",
    "set_print(sparsebert, True)\n",
    "set_output_masking(sparsebert, False)\n",
    "set_masking_timing(sparsebert, 'before_softmax')\n",
    "reset_input_mask(sparsebert)\n",
    "eval(sparsebert)\n",
    "update_input_mask(sparsebert)\n",
    "ret = eval(sparsebert)[:2]\n",
    "reset_input_mask(sparsebert)\n",
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:09<00:00, 105.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.93475"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(bert, eval=eval):\n",
    "    trainer.seed()\n",
    "    acc_sum = 0\n",
    "    N = 1000\n",
    "    for i in tqdm.tqdm(range(N)):\n",
    "        batch = trainer.get_batch(test=True)\n",
    "        output, label, _ = eval(bert, batch=batch)\n",
    "        acc_sum += torch.mean((output == label) * 1.0)\n",
    "    return acc_sum.item() / N\n",
    "accuracy(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:04<00:00, 15.58it/s]\n",
      "100%|██████████| 1000/1000 [01:03<00:00, 15.68it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('output mask: False', 0.92425, 0.9275)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sparse_eval(sparsebert, batch=batch):\n",
    "    reset_input_mask(sparsebert)\n",
    "    eval(sparsebert, batch=batch)\n",
    "    update_input_mask(sparsebert)\n",
    "    ret = eval(sparsebert, batch=batch)\n",
    "    return ret\n",
    "set_output_masking(sparsebert, False)\n",
    "set_masking_timing(sparsebert, 'after_softmax')\n",
    "set_print(sparsebert, False)\n",
    "acc_after = accuracy(sparsebert, eval=sparse_eval)\n",
    "set_print(sparsebert, True)\n",
    "set_masking_timing(sparsebert, 'before_softmax')\n",
    "set_print(sparsebert, False)\n",
    "acc_before = accuracy(sparsebert, eval=sparse_eval)\n",
    "set_print(sparsebert, True)\n",
    "'output mask: False', acc_after, acc_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:03<00:00, 15.74it/s]\n",
      "100%|██████████| 1000/1000 [01:04<00:00, 15.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('output mask: True', 0.915625, 0.924625)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_output_masking(sparsebert, True)\n",
    "set_masking_timing(sparsebert, 'after_softmax') #this must be wrong.\n",
    "set_print(sparsebert, False)\n",
    "acc_after = accuracy(sparsebert, eval=sparse_eval)\n",
    "set_print(sparsebert, True)\n",
    "set_masking_timing(sparsebert, 'before_softmax')\n",
    "set_print(sparsebert, False)\n",
    "acc_before = accuracy(sparsebert, eval=sparse_eval)\n",
    "set_print(sparsebert, True)\n",
    "'output mask: True', acc_after, acc_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention MSE, after softmax tensor(0.0025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Attention MSE, befo softmax tensor(0.0031, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "set_print(sparsebert, False)\n",
    "set_masking_timing(sparsebert, 'after_softmax')\n",
    "reset_input_mask(sparsebert)\n",
    "_,_, lm_output = eval(sparsebert, batch=batch)\n",
    "update_input_mask(sparsebert)\n",
    "_,_, lm_output_sparse = eval(sparsebert, batch=batch)\n",
    "reset_input_mask(sparsebert)\n",
    "print('Attention MSE, after softmax', torch.mean(torch.square(lm_output.attentions[-1]-lm_output_sparse.attentions[-1])))\n",
    "\n",
    "set_masking_timing(sparsebert, 'before_softmax')\n",
    "reset_input_mask(sparsebert)\n",
    "_,_, lm_output = eval(sparsebert, batch=batch)\n",
    "update_input_mask(sparsebert)\n",
    "_,_, lm_output_sparse = eval(sparsebert, batch=batch)\n",
    "reset_input_mask(sparsebert)\n",
    "print('Attention MSE, befo softmax', torch.mean(torch.square(lm_output.attentions[-1]-lm_output_sparse.attentions[-1])))\n",
    "set_print(sparsebert, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure input and output mask is same\n",
    "# set_output_masking(sparsebert, True)\n",
    "# reset_input_mask(sparsebert)\n",
    "# eval(sparsebert, batch=batch)\n",
    "# update_input_mask(sparsebert)\n",
    "# ret = eval(sparsebert, batch=batch)\n",
    "# for i in range(3):\n",
    "#     a = sparsebert.encoder.layer[i].attention.self.output_mask\n",
    "#     b = sparsebert.encoder.layer[i+1].attention.self.input_mask\n",
    "#     print(torch.mean(torch.abs(a-b)))\n",
    "# reset_input_mask(sparsebert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3052, -0.0182,  0.4914, -0.5295,  1.0349],\n",
      "        [ 0.1508, -0.2283, -0.1500, -0.2335,  0.7907],\n",
      "        [-0.6254, -0.0668, -0.5428, -0.7241,  1.5681],\n",
      "        [ 0.4267, -0.0869, -0.0414,  0.4975, -1.6607]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.4067, -0.2589,  0.5076, -0.1107,  0.9590],\n",
      "        [-0.1066, -0.5086,  0.3520,  0.1056,  0.3538],\n",
      "        [-1.0309, -0.5523, -0.5106, -0.6852,  2.1186],\n",
      "        [ 0.5187, -0.1235,  0.3521,  1.6831, -1.3839]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([1., 0., 0., 1.], device='cuda:0')\n",
      "tensor([1., 0., 0., 1.], device='cuda:0')\n",
      "tensor([[-0.0822, -0.4312,  0.5096,  0.0294,  1.0529],\n",
      "        [-0.3884, -0.4388,  0.1020,  0.4166,  0.2858],\n",
      "        [-0.8628, -0.5308, -0.6314, -0.2096,  1.2180],\n",
      "        [ 1.0807, -0.3690,  0.1572,  0.9194, -0.5309]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.4568, -0.4038,  0.3431, -0.0683,  0.7169],\n",
      "        [-0.0599, -0.3666,  0.0750,  0.3621, -0.2516],\n",
      "        [-0.3265, -0.7215, -0.8256, -0.4296,  1.4059],\n",
      "        [ 1.9299, -0.0835, -0.6393,  0.4410, -1.0698]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.3052, -0.0182,  0.4914, -0.5295,  1.0349],\n",
      "        [-0.5102,  0.0986,  0.1125, -0.0224,  0.8017],\n",
      "        [-1.0530,  0.4784, -0.7741, -0.4987,  1.3473],\n",
      "        [ 0.4267, -0.0869, -0.0414,  0.4975, -1.6607]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.4067, -0.2589,  0.5076, -0.1107,  0.9590],\n",
      "        [-0.2307, -0.1605,  0.5216,  0.0949,  0.1396],\n",
      "        [-1.2267,  0.4728, -0.5494, -0.6557,  1.7330],\n",
      "        [ 0.5187, -0.1235,  0.3521,  1.6831, -1.3839]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([1., 0., 0., 1.], device='cuda:0')\n",
      "tensor([1., 0., 0., 1.], device='cuda:0')\n",
      "tensor([[-0.0822, -0.4312,  0.5096,  0.0294,  1.0529],\n",
      "        [-0.2188,  0.1564,  0.3835,  0.0084,  0.1328],\n",
      "        [-0.9983,  0.7221, -0.5428, -0.5966,  1.4275],\n",
      "        [ 0.4055,  0.1916,  0.2055,  1.2778, -1.1371]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.4568, -0.4038,  0.3431, -0.0683,  0.7169],\n",
      "        [-0.0630,  0.0976,  0.2362,  0.2903,  0.1388],\n",
      "        [-0.5496,  0.8150, -0.6720, -0.8740,  1.3315],\n",
      "        [ 0.6600,  0.1397, -0.2790,  1.0046, -1.7808]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([1., 0., 0., 0.], device='cuda:0')\n",
      "tensor([1., 0., 0., 0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "set_print(sparsebert, False)\n",
    "set_masking_timing(sparsebert, 'before_softmax')\n",
    "set_output_masking(sparsebert, False)\n",
    "a = sparse_eval(sparsebert, batch=batch)[-1].hidden_states\n",
    "print(sparsebert.encoder.layer[0].self_attention_outputs[0][0][:4,:5])\n",
    "print(sparsebert.encoder.layer[1].attention.self.last_hidden_states[0,:4,:5])\n",
    "print(sparsebert.encoder.layer[0].attention.self.output_mask[0,:4])\n",
    "print(sparsebert.encoder.layer[1].attention.self.input_mask[0,:4])\n",
    "print(sparsebert.encoder.layer[1].self_attention_outputs[0][0][:4,:5])\n",
    "print(sparsebert.encoder.layer[2].attention.self.last_hidden_states[0,:4,:5])\n",
    "set_output_masking(sparsebert, True)\n",
    "b = sparse_eval(sparsebert, batch=batch)[-1].hidden_states\n",
    "b[-1].shape, torch.mean(torch.abs(a[-1]-b[-1]))\n",
    "b[-1][0][1] # even if i mask output, the result is not 0 because of layernorm? (I am not sure)\n",
    "print(sparsebert.encoder.layer[0].self_attention_outputs[0][0][:4,:5])\n",
    "print(sparsebert.encoder.layer[1].attention.self.last_hidden_states[0,:4,:5])\n",
    "print(sparsebert.encoder.layer[0].attention.self.output_mask[0,:4])\n",
    "print(sparsebert.encoder.layer[1].attention.self.input_mask[0,:4])\n",
    "print(sparsebert.encoder.layer[1].self_attention_outputs[0][0][:4,:5])\n",
    "print(sparsebert.encoder.layer[2].attention.self.last_hidden_states[0,:4,:5])\n",
    "print(sparsebert.encoder.layer[1].attention.self.output_mask[0,:4])\n",
    "print(sparsebert.encoder.layer[2].attention.self.input_mask[0,:4])\n",
    "#sparsebert.encoder.layer[-1].attention.self.output_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approx attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120000/120000 [00:00<00:00, 214666.85it/s]\n",
      "100%|██████████| 7600/7600 [00:00<00:00, 199999.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Dataset Stat.: name:AG_NEWS, nclass:5, max_len:1012, avg_len:236.477525, count:120000\n",
      "Classification Dataset Stat.: name:AG_NEWS, nclass:5, max_len:892, avg_len:235.2992105263158, count:7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-4_H-256_A-4 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer.__init__: Model initialized. model = bert\n",
      "Trainer.load: Loading...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 32, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 32)\n",
       "    (token_type_embeddings): Embedding(2, 32)\n",
       "    (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (key): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (value): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=32, out_features=1024, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=32, bias=True)\n",
       "          (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (key): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (value): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=32, out_features=1024, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=32, bias=True)\n",
       "          (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (key): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (value): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=32, out_features=1024, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=32, bias=True)\n",
       "          (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (key): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (value): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=32, out_features=1024, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=32, bias=True)\n",
       "          (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trainer.attention_approx import Trainer as ApproxTrainer\n",
    "approx_trainer = ApproxTrainer(factor=8)\n",
    "approx_trainer.load()\n",
    "approx_trainer.bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2247,  0.0000,  1.2247],\n",
       "         [ 0.7071,  0.7071, -1.4142]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.layer_norm(torch.tensor([[[1,2,3],[3,48,-99999991]]], dtype=torch.float32), (3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-93-75e71dbf1cbe>:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  F.softmax(torch.tensor([-10000,1,0.5], dtype=torch.float32))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.6225, 0.3775])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(torch.tensor([-10000,1,0.5], dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[    -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0., -10000.,\n",
       "           -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
       "           -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
       "           -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
       "           -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
       "           -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
       "           -10000.]]],\n",
       "\n",
       "\n",
       "        [[[    -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0., -10000., -10000., -10000., -10000.,\n",
       "           -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
       "           -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
       "           -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
       "           -10000.]]],\n",
       "\n",
       "\n",
       "        [[[    -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.]]],\n",
       "\n",
       "\n",
       "        [[[    -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0., -10000., -10000., -10000., -10000.,\n",
       "           -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
       "           -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
       "           -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
       "           -10000.]]],\n",
       "\n",
       "\n",
       "        [[[    -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0., -10000., -10000., -10000., -10000., -10000.,\n",
       "           -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
       "           -10000.]]],\n",
       "\n",
       "\n",
       "        [[[    -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0., -10000., -10000.,\n",
       "           -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
       "           -10000.]]],\n",
       "\n",
       "\n",
       "        [[[    -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
       "           -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
       "           -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
       "           -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
       "           -10000.]]],\n",
       "\n",
       "\n",
       "        [[[    -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0.,     -0.,\n",
       "               -0.,     -0.,     -0.,     -0.,     -0.,     -0., -10000.,\n",
       "           -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
       "           -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
       "           -10000., -10000., -10000., -10000., -10000., -10000., -10000.,\n",
       "           -10000.]]]], device='cuda:0')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsebert.encoder.layer[0].attention.self.last_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7b3ac0126d0d6fea024471ce24e510948bf6332f7ae1a66cdcb4ee9887514e9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('tensorflow': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
